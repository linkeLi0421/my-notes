{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Random Notes about what I learn A growing, searchable notebook organized by date. Browse by year in the sidebar or use search to jump straight in."},{"location":"#start-here","title":"Start here","text":"<ul> <li>Use the left sidebar to navigate by year and month.</li> <li>Use search for quick recall across everything.</li> <li>New notes appear automatically when you add files under <code>notes/</code>.</li> </ul>"},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/","title":"Epstein RAG \u9879\u76ee\u5b66\u4e60\u7b14\u8bb0","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#1-asyncawait","title":"1. async/await \u6838\u5fc3\u6982\u5ff5","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#asyncawait","title":"\u4ec0\u4e48\u662f async/await\uff1f","text":"<ul> <li>async def\uff1a\u58f0\u660e\u4e00\u4e2a\u5f02\u6b65\u51fd\u6570\uff08\u534f\u7a0b\uff09\uff0c\u4e0d\u4f1a\u7acb\u523b\u6267\u884c\uff0c\u8fd4\u56de\u534f\u7a0b\u5bf9\u8c61</li> <li>await\uff1a\u5728\u5f02\u6b65\u51fd\u6570\u5185\u90e8\u7b49\u5f85\u67d0\u4e2a I/O \u64cd\u4f5c\u5b8c\u6210</li> <li>\u540c\u6b65\u7b49\u5f85\uff1a\u7ebf\u7a0b\u88ab\u5361\u4f4f\uff0c\u4ec0\u4e48\u90fd\u4e0d\u80fd\u5207\u6362</li> <li>async \u7b49\u5f85\uff1a\u5f53\u524d\u534f\u7a0b\u6682\u505c\uff0c\u63a7\u5236\u6743\u8fd8\u7ed9\u4e8b\u4ef6\u5faa\u73af\uff1b\u5176\u4ed6\u4efb\u52a1\u53ef\u4ee5\u7ee7\u7eed\u8dd1</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#asyncawait_1","title":"\u4e3a\u4ec0\u4e48\u8981\u7528 async/await\uff1f","text":"<p>\u8fd9\u4e2a\u9879\u76ee\u662f MCP \u670d\u52a1\u7aef\uff0c\u9700\u8981\uff1a - \u6301\u7eed\u5904\u7406\u591a\u4e2a\u5e76\u53d1\u8bf7\u6c42\uff08\u6570\u636e\u5e93\u67e5\u8be2\u3001\u5411\u91cf\u68c0\u7d22\u3001\u65e5\u5fd7\u5199\u5165\uff09 - \u6bcf\u4e2a\u64cd\u4f5c\u90fd\u6d89\u53ca I/O\uff08\u6570\u636e\u5e93\u3001\u7f51\u7edc\u3001\u6807\u51c6\u8f93\u5165\u8f93\u51fa\uff09 - \u5f02\u6b65\u6a21\u578b\u8ba9\u540c\u4e00\u7ebf\u7a0b\u5728\u7b49\u5f85 I/O \u65f6\u5904\u7406\u5176\u4ed6\u4efb\u52a1\uff0c\u63d0\u9ad8\u6548\u7387</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#3","title":"3 \u6761\u8bb0\u4f4f\u7684\u89c4\u5219","text":"<ol> <li><code>await</code> \u53ea\u80fd\u5199\u5728 <code>async def</code> \u91cc</li> <li>\u9047\u5230 I/O\uff08DB\u3001\u7f51\u7edc\u3001\u6d41\uff09\u4f18\u5148\u8003\u8651\u5f02\u6b65\u63a5\u53e3</li> <li>\u6700\u5916\u5c42\u7528 <code>asyncio.run(...)</code> \u542f\u52a8\u4e8b\u4ef6\u5faa\u73af</li> </ol>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#2-mcp-server-serverpy","title":"2. MCP Server \u67b6\u6784\uff08server.py\uff09","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_1","title":"\u6587\u4ef6\u8def\u5f84","text":"<p><code>mcp_server/server.py</code> - MCP \u670d\u52a1\u5165\u53e3</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_2","title":"\u5173\u952e\u5bf9\u8c61","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#app-serverconfigserver_name","title":"app = Server(config.server_name)","text":"<ul> <li>\u7c7b\u578b\uff1a<code>mcp.server.Server</code> \u5b9e\u4f8b</li> <li>\u4f5c\u7528\uff1aMCP \u670d\u52a1\u672c\u4f53\uff0c\u7ba1\u7406\u6240\u6709\u5de5\u5177\u548c\u8d44\u6e90</li> <li>\u4e0d\u662f HTTP \u5e94\u7528\uff0c\u800c\u662f\u901a\u8fc7 stdio \u901a\u4fe1\u7684\u534f\u8bae\u670d\u52a1\u5668</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#rag-ragengine","title":"rag = RAGEngine()","text":"<ul> <li>\u7c7b\u578b\uff1a<code>mcp_server.rag_engine.RAGEngine</code> \u5b9e\u4f8b</li> <li>\u4f5c\u7528\uff1a\u5411\u91cf\u68c0\u7d22\u3001\u6587\u6863\u7d22\u5f15\u7684\u6838\u5fc3\u5f15\u64ce</li> <li>\u8d1f\u8d23\uff1achunk\u3001embedding\u3001ChromaDB \u67e5\u8be2</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#mcp-sdk","title":"\u88c5\u9970\u5668\u7cfb\u7edf\uff08MCP SDK \u9884\u5b9a\u4e49\u7684\u6ce8\u518c\u94a9\u5b50\uff09","text":"\u88c5\u9970\u5668 \u6ce8\u518c\u7684\u51fd\u6570 \u4f5c\u7528 <code>@app.list_tools()</code> <code>list_tools()</code> \u8fd4\u56de\u53ef\u7528\u5de5\u5177\u5217\u8868 <code>@app.call_tool()</code> <code>call_tool(name, arguments)</code> \u6267\u884c\u6307\u5b9a\u5de5\u5177 <code>@app.list_resources()</code> <code>list_resources()</code> \u8fd4\u56de\u8d44\u6e90\u5217\u8868 <code>@app.read_resource()</code> <code>read_resource(uri)</code> \u8bfb\u53d6\u8d44\u6e90\u5185\u5bb9","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#mcp-ai-agent","title":"\u66b4\u9732\u7684 MCP \u5de5\u5177\uff08\u7ed9 AI Agent \u7528\uff09","text":"<pre><code>TOOLS = [\n  - index_documents(folder_path)          # \u7d22\u5f15\u6587\u4ef6\u5939\n  - query_documents(query, top_k=5)       # RAG \u67e5\u8be2\n  - search_similar(query, top_k=5)        # \u76f8\u4f3c\u5ea6\u641c\u7d22\n  - get_document_summary(source)          # \u83b7\u53d6\u6587\u6863\u6458\u8981\n  - list_indexed_documents()              # \u5217\u51fa\u5df2\u7d22\u5f15\u6587\u6863\n  - delete_document(source)               # \u5220\u9664\u6587\u6863\n  - reset_index()                         # \u91cd\u7f6e\u7d22\u5f15\n  - check_status()                        # \u68c0\u67e5\u7cfb\u7edf\u72b6\u6001\n]\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#mcp","title":"\u66b4\u9732\u7684 MCP \u8d44\u6e90","text":"<pre><code>RESOURCES = [\n  - stats://queries      # \u67e5\u8be2\u7edf\u8ba1\n  - stats://documents    # \u6587\u6863\u7edf\u8ba1\n  - stats://jobs         # \u4efb\u52a1\u7edf\u8ba1\n  - stats://system       # \u7cfb\u7edf\u5065\u5eb7\n]\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#3_1","title":"3. \u6267\u884c\u6d41\u7a0b\u89e3\u6790","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#main","title":"\u5b8c\u6574\u7684\u542f\u52a8\u987a\u5e8f\uff08main() \u51fd\u6570\uff09","text":"<pre><code>async def main():\n    # 1. \u521d\u59cb\u5316\u6570\u636e\u5e93\u8868\n    logger.info(\"Initialising database tables...\")\n    await init_db()  # \u7b49\u5f85\u6570\u636e\u5e93\u64cd\u4f5c\u5b8c\u6210\n\n    # 2. \u542f\u52a8 MCP \u670d\u52a1\n    logger.info(\"Starting MCP server '%s'...\", config.server_name)\n\n    # 3. \u8fdb\u5165\u65e0\u9650\u5faa\u73af\uff08\u7b49\u5f85\u5ba2\u6237\u7aef\u6d88\u606f\uff09\n    async with stdio_server() as (read_stream, write_stream):\n        await app.run(read_stream, write_stream, app.create_initialization_options())\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_3","title":"\u65f6\u95f4\u7ebf","text":"<ol> <li>\u542f\u52a8\u9636\u6bb5\uff08\u987a\u5e8f\u6267\u884c\uff09</li> <li>Log 1: \"Initialising database tables...\"</li> <li>\u6267\u884c <code>await init_db()</code>\uff08DB \u5efa\u8868\uff09</li> <li> <p>Log 2: \"Starting MCP server...\"</p> </li> <li> <p>\u8fd0\u884c\u9636\u6bb5\uff08\u5e76\u53d1\u5faa\u73af\uff09</p> </li> <li>\u8fdb\u5165 <code>async with stdio_server()</code> \u521b\u5efa stdin/stdout \u6d41</li> <li><code>await app.run(...)</code> \u542f\u52a8\u4e8b\u4ef6\u5faa\u73af</li> <li>\u6301\u7eed\u7b49\u5f85 \u2192 \u5904\u7406\u8bf7\u6c42 \u2192 \u56de\u5305 \u2192 \u7ee7\u7eed\u7b49\u5f85...</li> </ol>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_4","title":"\u5173\u952e\u70b9","text":"<ul> <li>\u542f\u52a8\u9636\u6bb5\u662f\"\u5148\u7b49 init_db \u5b8c\u6210\uff0c\u518d\u7ee7\u7eed\"\uff08\u4e32\u884c\uff09</li> <li>\u8fd0\u884c\u9636\u6bb5\u662f\"\u6301\u7eed\u76d1\u542c\u591a\u4e2a\u5e76\u53d1\u8bf7\u6c42\"\uff08\u5e76\u884c\uff09</li> <li>\u771f\u6b63\u4f53\u73b0\u5f02\u6b65\u4f18\u52bf\u7684\u662f <code>app.run</code> \u7684\u65e0\u9650\u5faa\u73af\u91cc</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#4","title":"4. \u6838\u5fc3\u6982\u5ff5\u89e3\u91ca","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#stdio_server","title":"stdio_server \u662f\u4ec0\u4e48\uff1f","text":"<ul> <li>\u4e00\u4e2a\"\u5f02\u6b65\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\"\uff08async context manager\uff09</li> <li>\u4f5c\u7528\uff1a\u628a\u7ec8\u7aef\u7684\u6807\u51c6\u8f93\u5165/\u8f93\u51fa\u5305\u88c5\u6210\u53ef\u5f02\u6b65\u8bfb\u5199\u7684\u6d41</li> <li>MCP \u5ba2\u6237\u7aef\u901a\u8fc7 stdio \u53d1\u9001 JSON-RPC \u6d88\u606f\uff0c\u670d\u52a1\u7aef\u901a\u8fc7 stdio \u56de\u590d</li> <li>\u8fd9\u4e0d\u662f HTTP \u7aef\u53e3\u670d\u52a1\uff0c\u800c\u662f\"\u8fdb\u7a0b\u95f4\u7ba1\u9053\u901a\u4fe1\"</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#asynciorunmain","title":"asyncio.run(main()) \u7684\u4f5c\u7528","text":"<ol> <li>\u521b\u5efa\u4e8b\u4ef6\u5faa\u73af</li> <li>\u628a <code>main()</code> \u534f\u7a0b\u653e\u8fdb\u53bb\u8fd0\u884c</li> <li>\u7b49 <code>main()</code> \u5b8c\u6210\uff08\u901a\u5e38\u6c38\u4e0d\u5b8c\u6210\uff0c\u56e0\u4e3a <code>app.run</code> \u65e0\u9650\u5faa\u73af\uff09</li> <li>\u9000\u51fa\u65f6\u5173\u95ed\u4e8b\u4ef6\u5faa\u73af\u548c\u6240\u6709\u4efb\u52a1</li> </ol> <p>\u53ef\u4ee5\u7406\u89e3\u6210\u5f02\u6b65\u7a0b\u5e8f\u7684\"\u603b\u5f00\u5173\"\uff0c\u7c7b\u4f3c\u540c\u6b65\u7a0b\u5e8f\u7684 startup \u51fd\u6570\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#apprun","title":"\u4e3a\u4ec0\u4e48\u8bf4 app.run \u662f\u4e00\u4e2a\u5faa\u73af\uff1f","text":"<pre><code>app.run(...) \u5185\u90e8\u6301\u7eed\u505a\uff1a\n  \u2193\n\u7b49\u5f85\u4e00\u4e2a\u6d88\u606f\u4ece stdin \u8fdb\u6765\n  \u2193\n\u8bc6\u522b\u6d88\u606f\u7c7b\u578b\uff08list_tools? call_tool? read_resource?\uff09\n  \u2193\n\u8def\u7531\u5230\u5bf9\u5e94\u7684\u88c5\u9970\u5668\u51fd\u6570\uff08@app.call_tool() \u7b49\uff09\n  \u2193\n\u6267\u884c\u5b8c\u6bd5\uff0c\u628a\u7ed3\u679c JSON \u5199\u5230 stdout\n  \u2193\n\u56de\u5230\"\u7b49\u5f85\u4e00\u4e2a\u6d88\u606f\"\n  \u2193\n(\u5faa\u73af\u76f4\u5230\u8fdb\u7a0b\u88ab\u6740\u6b7b)\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#5","title":"5. \u6570\u636e\u6d41\u5411\u603b\u7ed3","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#query_documents","title":"query_documents \u8bf7\u6c42\u4e3e\u4f8b","text":"<pre><code>Claude Desktop (\u5ba2\u6237\u7aef)\n  \u2193\n\u901a\u8fc7 MCP protocol \u53d1\u9001 JSON-RPC \u8bf7\u6c42\u5230 stdio\n  \u2193\nstdio_server \u63a5\u6536\u6d88\u606f\n  \u2193\napp.run() \u8bc6\u522b\u662f \"call_tool\" \u8bf7\u6c42\uff0c\u5de5\u5177\u540d \"query_documents\"\n  \u2193\n@app.call_tool() \u88c5\u9970\u5668\u6355\u83b7\n  \u2193\n_dispatch_tool(name=\"query_documents\", arguments={...})\n  \u2193\n_tool_query_documents(args)\n  \u2193\n  1. \u8ba1\u65f6\u5f00\u59cb QueryTimer()\n  2. await rag.query() \u5411 ChromaDB \u68c0\u7d22\n  3. \u7ec4\u88c5\u54cd\u5e94\u6587\u672c\n  4. await log_query() \u5199\u5165 PostgreSQL\n  \u2193\n\u628a\u7ed3\u679c JSON \u5199\u56de stdout\n  \u2193\nClaude Desktop \u6536\u5230\u54cd\u5e94\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#6","title":"6. \u5173\u952e\u6587\u4ef6\u5bfc\u822a","text":"\u6587\u4ef6 \u4f5c\u7528 <code>mcp_server/server.py</code> MCP \u670d\u52a1\u5165\u53e3\u3001\u5de5\u5177\u5b9a\u4e49 <code>mcp_server/rag_engine.py</code> \u5411\u91cf\u68c0\u7d22\u3001\u6587\u6863\u7d22\u5f15 <code>mcp_server/logging_utils.py</code> \u65e5\u5fd7\u5199\u5165 PostgreSQL <code>mcp_server/models.py</code> SQLAlchemy \u6570\u636e\u6a21\u578b <code>mcp_server/config.py</code> \u73af\u5883\u53d8\u91cf\u914d\u7f6e <code>services/pipeline.py</code> \u79bb\u7ebf\u6279\u5904\u7406\u5efa\u5e93 <code>dashboard_backend/main.py</code> Dashboard API \u5165\u53e3 <code>dashboard_frontend/src/App.tsx</code> \u524d\u7aef\u8def\u7531","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#7-rag-engine-rag_enginepy","title":"7. RAG Engine \u6df1\u5ea6\u89e3\u6790\uff08rag_engine.py\uff09","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_5","title":"\u6587\u4ef6\u6982\u89c8","text":"<p><code>mcp_server/rag_engine.py</code> - \u5411\u91cf\u5b58\u50a8\u3001\u6587\u6863\u7d22\u5f15\u3001\u8bed\u4e49\u68c0\u7d22\u6838\u5fc3</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#lazy-initialization","title":"\u8bbe\u8ba1\u6a21\u5f0f\uff1aLazy Initialization","text":"<pre><code>class RAGEngine:\n    def __init__(self):\n        self._client = None        # \u4e0d\u5728 __init__ \u65f6\u8fde\u63a5\n        self._collection = None\n        self._model = None\n\n    def _get_client(self):\n        if self._client is None:        # \u5ef6\u8fdf\u521d\u59cb\u5316\n            self._client = chromadb.HttpClient(...)\n        return self._client\n</code></pre> <p>\u597d\u5904\uff1a - \u52a0\u5feb\u542f\u52a8\u901f\u5ea6\uff08\u4e0d\u9700\u8981\u7b49 ChromaDB \u8fde\u63a5\u3001\u52a0\u8f7d\u6a21\u578b\uff09 - \u907f\u514d\u542f\u52a8\u65f6\u56e0\u8fde\u63a5\u5931\u8d25\u5bfc\u81f4\u6574\u4e2a\u8fdb\u7a0b\u5d29\u6e83 - \u53ea\u5728\u771f\u6b63\u4f7f\u7528\u65f6\u624d\u521d\u59cb\u5316\u8d44\u6e90</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#1chunking","title":"\u6838\u5fc3\u6982\u5ff5 1\uff1a\u5206\u5757\uff08Chunking\uff09","text":"<p>\u76ee\u7684\uff1a\u628a\u5927\u6587\u6863\u5206\u6210\u53ef\u7ba1\u7406\u7684\u5c0f\u5757\uff0c\u540c\u65f6\u4fdd\u7559\u4e0a\u4e0b\u6587</p> <pre><code>\u539f\u6587\u672c\uff1a\u957f 100,000 \u5b57\n\n\u5206\u5757\u903b\u8f91\uff1a\n  chunk_size = 1000\n  chunk_overlap = 200\n\n  \u7b2c 1 \u5757\uff1a[0, 1000]\n  \u7b2c 2 \u5757\uff1a[800, 1800]    \u2190 \u91cd\u53e0 200 \u5b57\n  \u7b2c 3 \u5757\uff1a[1600, 2600]   \u2190 \u91cd\u53e0 200 \u5b57\n  ...\n\n\u597d\u5904\uff1a\n  - \"\u5c9b\u4e0a\u7684\u8bbf\u5ba2\"\u4e0d\u4f1a\u88ab\u5272\u6210\"\u5c9b\u4e0a\u7684\"\u548c\"\u8bbf\u5ba2\"\n  - \u6570\u636e\u5e93\u4e0d\u4f1a\u56e0\u5de8\u5927\u5411\u91cf\u800c\u7206\u70b8\n</code></pre> <p>\u7ed3\u6784\uff08mcp_server/rag_engine.py\uff09\uff1a <pre><code>_chunk_text() \u8fd4\u56de\uff1a\n[\n  {\n    \"id\": \"file.pdf_ae3f1\",                           # \u552f\u4e00\u6807\u8bc6\uff08hash \u751f\u6210\uff09\n    \"text\": \"The flight logs show...\",                 # \u6587\u672c\u5185\u5bb9\n    \"metadata\": {\n      \"source\": \"file.pdf\",\n      \"chunk_index\": 0,\n      \"char_start\": 0,\n      \"char_end\": 1000\n    }\n  },\n  ...\n]\n</code></pre></p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#2embedding","title":"\u6838\u5fc3\u6982\u5ff5 2\uff1aEmbedding\uff08\u5411\u91cf\u5316\uff09","text":"<p>\u76ee\u7684\uff1a\u628a\u6587\u672c\u8f6c\u6210\u6570\u5b66\u5411\u91cf\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8868\u793a\u8bed\u4e49</p> <pre><code>\u8f93\u5165\uff1a\u6587\u672c \"flight logs\"\n           \u2193\nSentenceTransformer (all-MiniLM-L6-v2)\n           \u2193\n\u8f93\u51fa\uff1a384 \u7ef4\u6d6e\u70b9\u5411\u91cf\n  [0.12, -0.03, 0.45, 0.33, ..., -0.15]\n</code></pre> <p>\u5173\u952e\u70b9\uff1a - \u540c\u6837\u542b\u4e49\u7684\u6587\u672c \u2192 \u76f8\u4f3c\u7684\u5411\u91cf \u2192 \"\u8ddd\u79bb\"\u5c0f - 384 \u7ef4\u662f\u5e73\u8861\u70b9\uff1a\u7ec6\u8282\u4e30\u5bcc\uff0c\u8ba1\u7b97\u6210\u672c\u4e0d\u8fc7\u9ad8 - SentenceTransformer \u662f\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5df2\u5b66\u4f1a\u8bed\u4e49\u6620\u5c04</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#3chromadb","title":"\u6838\u5fc3\u6982\u5ff5 3\uff1aChromaDB\uff08\u5411\u91cf\u6570\u636e\u5e93\uff09","text":"<p>\u662f\u4ec0\u4e48\uff1a\u5b58\u50a8\u5411\u91cf + \u6587\u672c + \u5143\u6570\u636e\uff0c\u652f\u6301\u5feb\u901f\u76f8\u4f3c\u5ea6\u641c\u7d22</p> <pre><code>Collection: \"epstein_documents\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id          | text          | metadata | embedding\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502file.pdf_ae3f1\u2502\"The flight...\"\u2502{source..}\u2502[0.12...]\u2502\n\u2502file.pdf_b2d7e\u2502\"[continued]...\"\u2502{source..}\u2502[0.15...]\u2502\n\u2502file2.md_c5f3a\u2502\"At the island\"\u2502{source..}\u2502[0.18...]\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>HTTP \u67b6\u6784\uff1a <pre><code>MCP Server (localhost:5001)\n    \u2193 HTTP\nChromaDB (localhost:8000)\n    \u2193 \u78c1\u76d8\n\u5411\u91cf\u7d22\u5f15\u6587\u4ef6 + \u5143\u6570\u636e\n</code></pre></p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_6","title":"\u7d22\u5f15\u6d41\u7a0b\uff08\u5efa\u5e93\uff09","text":"<p>mcp_server/rag_engine.py\uff1a</p> <pre><code>index_folder()\n  \u2193\n\u904d\u5386\u6240\u6709 .txt, .md, .pdf \u6587\u4ef6\n  \u2193\n\u5bf9\u6bcf\u4e2a\u6587\u4ef6\uff1aindex_file()\n  \u251c\u2500 \u8bfb\u6587\u4ef6\u5185\u5bb9\n  \u251c\u2500 _chunk_text() \u5206\u5757\uff08\u5e26 200 \u5b57\u91cd\u53e0\uff09\n  \u251c\u2500 model.encode() \u751f\u6210 384 \u7ef4\u5411\u91cf\n  \u251c\u2500 collection.upsert(\n  \u2502    ids=[...],\n  \u2502    documents=[...],\n  \u2502    metadatas=[...],\n  \u2502    embeddings=[...]\n  \u2514\u2500 )\n</code></pre> <p>upsert \u7279\u70b9\uff1a - id \u5b58\u5728 \u2192 \u66f4\u65b0\uff08\u8986\u76d6\uff09 - id \u4e0d\u5b58\u5728 \u2192 \u63d2\u5165 - \u91cd\u65b0\u7d22\u5f15\u540c\u4e00\u6587\u4ef6\u65e0\u9700\u624b\u52a8\u5220\u9664</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_7","title":"\u67e5\u8be2\u6d41\u7a0b\uff08\u68c0\u7d22\uff09","text":"<p>mcp_server/rag_engine.py\uff1a</p> <pre><code>query(\"who visited the island?\", top_k=5)\n  \u2193\n1. \u628a\u67e5\u8be2\u6587\u672c\u4e5f\u8f6c\u6210 384 \u7ef4\u5411\u91cf\n   model.encode([\"who visited...\"])\n   \u2193\n2. ChromaDB \u641c\u7d22\uff08cosine \u8ddd\u79bb\uff09\n   collection.query(\n     query_embeddings=[...],\n     n_results=5,\n     include=[\"documents\", \"metadatas\", \"distances\"]\n   )\n   \u2193\n3. \u5185\u90e8\u7528 HNSW \u7d22\u5f15\u627e\u6700\u8fd1\u7684 5 \u4e2a\u5411\u91cf\n   \u2193\n4. \u8fd4\u56de\uff1a\n   {\n     \"ids\": [[\"file.pdf_ae3f1\", \"file.pdf_b2d7e\", ...]],\n     \"documents\": [[\"text1\", \"text2\", ...]],\n     \"distances\": [[0.05, 0.12, 0.18, ...]]\n   }\n   \u2193\n5. \u8f6c\u5316\u8ddd\u79bb\u4e3a\u76f8\u4f3c\u5ea6\n   similarity = 1 - distance\n   [0.95, 0.88, 0.82, ...]\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#vs","title":"\u8ddd\u79bb vs \u76f8\u4f3c\u5ea6","text":"\u6982\u5ff5 \u8bf4\u6cd5 \u8303\u56f4 \u610f\u4e49 \u8ddd\u79bb (ChromaDB) \"cosine distance\" [0, 2] \u8d8a\u5c0f\u8d8a\u76f8\u4f3c \u76f8\u4f3c\u5ea6 (\u5e94\u7528\u5c42) \"similarity score\" [0, 1] \u8d8a\u5927\u8d8a\u76f8\u4f3c <pre><code>distance = 0.05  \u2192  similarity = 1 - 0.05 = 0.95 \u2b50\ndistance = 0.50  \u2192  similarity = 1 - 0.50 = 0.50 \ud83e\udd14\ndistance = 1.00  \u2192  similarity = 1 - 1.00 = 0.00 \u274c\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_8","title":"\u6587\u6863\u7ba1\u7406","text":"\u64cd\u4f5c \u4ee3\u7801 \u4f5c\u7528 \u5217\u8868 <code>collection.get(include=[\"metadatas\"])</code> \u904d\u5386\u6240\u6709\u6587\u6863\u540d \u5220\u9664 <code>collection.get(where={\"source\": \"file.pdf\"}); delete()</code> \u5220\u9664\u5355\u4e2a\u6587\u4ef6\u7684\u6240\u6709 chunk \u91cd\u7f6e <code>client.delete_collection()</code> \u6e05\u7a7a\u6574\u4e2a\u7d22\u5f15\u5e93 \u72b6\u6001 <code>collection.count()</code> \u67e5\u770b\u5df2\u7d22\u5f15 chunk \u6570","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_9","title":"\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56","text":"\u95ee\u9898 \u7b54\u6848 \u539f\u56e0 \u4e3a\u4ec0\u4e48\u5206\u5757\u6709\u91cd\u53e0\uff1f \u907f\u514d\u8bed\u4e49\u65ad\u88c2 \"\u5c9b\u4e0a\u7684\u8bbf\u5ba2\"\u88ab\u5206\u6210\"\u5c9b\u4e0a\u7684\"+\"\u8bbf\u5ba2\"\u5c31\u4e22\u610f\u4e49\u4e86 384 \u7ef4\u800c\u4e0d\u662f 10 \u7ef4\uff1f \u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387 \u7ef4\u5ea6\u8d8a\u9ad8\u8d8a\u7cbe\u7ec6\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8d8a\u9ad8 \u4e3a\u4ec0\u4e48\u7528 HTTP ChromaDB\uff1f \u652f\u6301\u5206\u5e03\u5f0f + \u6301\u4e45\u5316 Docker \u5bb9\u5668\u53ef\u72ec\u7acb\u6269\u5c55\uff0c\u6570\u636e\u4e0d\u968f\u8fdb\u7a0b\u4e22\u5931 \u4e3a\u4ec0\u4e48 cosine \u8ddd\u79bb\uff1f \u5bf9\u6587\u672c\u6700\u53cb\u597d \u6587\u672c\u5411\u91cf\u591a\u6570\u9ad8\u7ef4\uff0ccosine \u9002\u5408\u9ad8\u7ef4\u7a7a\u95f4","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_10","title":"\u5b8c\u6574\u6570\u636e\u6d41\uff08\u4ece\u6587\u4ef6\u5230\u7b54\u6848\uff09","text":"<pre><code>\u7528\u6237\u6587\u4ef6\uff1a/data/docs/flight_logs.pdf\n  \u2193\nindex_documents(\"/data/docs\")\n  \u251c\u2500 \u5206\u5757\uff1a[chunk1, chunk2, chunk3, ...]\n  \u251c\u2500 \u5411\u91cf\u5316\uff1a\u6bcf\u4e2a chunk \u751f\u6210 384 \u7ef4\u5411\u91cf\n  \u2514\u2500 ChromaDB \u5b58\u50a8\n       \u2193\n    query_logs \u8868\n  ({file: \"flight_logs.pdf\", chunks: 245, indexed_at: \"2026-02-23\"})\n      \u2193\n\u7528\u6237\u63d0\u95ee\uff1a\"who visited the island?\"\n  \u2193\nquery_documents(\"who visited...\")\n  \u251c\u2500 \u5411\u91cf\u5316\u67e5\u8be2\uff1a[0.10, -0.02, 0.47, ...]\n  \u251c\u2500 ChromaDB \u641c\u7d22\uff1atop-5 \u76f8\u4f3c chunk\n  \u251c\u2500 \u8fd4\u56de\u7ed3\u679c\uff1a[{text: \"...\", similarity: 0.95, source: \"flight_logs.pdf\"}, ...]\n  \u251c\u2500 log_query() \u5199\u65e5\u5fd7\u5230 PostgreSQL\n  \u2514\u2500 \u8fd4\u56de\u7ed9 Claude Desktop\n</code></pre>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#8-sentence-transformers","title":"8. Sentence-Transformers\uff08\u53e5\u5411\u91cf\u6a21\u578b\uff09","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_11","title":"\u5b83\u5728\u672c\u9879\u76ee\u91cc\u7684\u89d2\u8272","text":"<p>SentenceTransformer \u8d1f\u8d23\u628a\u6587\u672c\u7f16\u7801\u6210\u5411\u91cf\uff08embedding\uff09\uff0c\u7528\u4e8e\uff1a - \u5efa\u5e93\uff1a\u5bf9\u6bcf\u4e2a chunk \u8c03\u7528 <code>model.encode(texts)</code> \u2192 \u5b58\u5165 ChromaDB - \u67e5\u8be2\uff1a\u5bf9 query \u8c03\u7528 <code>model.encode([query_text])</code> \u2192 \u7528\u5411\u91cf\u8fd1\u90bb\u641c\u7d22\u627e\u5230\u6700\u76f8\u4f3c chunk</p> <p>\u5bf9\u5e94\u4ee3\u7801\uff1a - \u6a21\u578b\u52a0\u8f7d\uff1amcp_server/rag_engine.py - \u5efa\u5e93\u5411\u91cf\u5316\uff1amcp_server/rag_engine.py - \u67e5\u8be2\u5411\u91cf\u5316\uff1amcp_server/rag_engine.py</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#structure","title":"\u7ed3\u6784\uff08Structure\uff09\uff1a\u6a21\u5757\u5316\u6d41\u6c34\u7ebf","text":"<p>\u4e00\u4e2a\u5178\u578b Sentence-Transformers \u6a21\u578b\u53ef\u4ee5\u770b\u4f5c\uff1a</p> <pre><code>Text\n  \u2193\nTokenizer\uff08\u5206\u8bcd\u6210 token ids\uff09\n  \u2193\nTransformer Encoder\uff08\u5982 MiniLM/BERT\uff0c\u8f93\u51fa\u6bcf\u4e2a token \u7684\u5411\u91cf\uff09\n  \u2193\nPooling\uff08\u628a token-level \u5411\u91cf\u805a\u5408\u6210 sentence embedding\uff09\n  \u2193\n(\u53ef\u9009) Dense/Projection\uff08\u6295\u5f71/\u964d\u7ef4\uff09\n  \u2193\n(\u5e38\u89c1) Normalize\uff08\u5411\u91cf\u5f52\u4e00\u5316\uff0c\u4fbf\u4e8e cosine \u76f8\u4f3c\u5ea6\uff09\n  \u2193\nSentence Embedding\uff08\u56fa\u5b9a\u7ef4\u5ea6\uff0c\u5982 384 \u7ef4\uff09\n</code></pre> <p>\u8bb0\u53f7\u5316\u63cf\u8ff0\uff1a - Encoder \u8f93\u51fa\uff1a\\(H \\in \\mathbb{R}^{L \\times d}\\)\uff08\\(L\\) token \u6570\uff0c\\(d\\) \u9690\u85cf\u7ef4\u5ea6\uff09 - Mean Pooling \u5e38\u89c1\u5f62\u5f0f\uff1a</p> \\[ s = \\frac{\\sum_{i=1}^{L} m_i H_i}{\\sum_{i=1}^{L} m_i} \\] <p>\u5176\u4e2d \\(m_i\\) \u662f mask\uff08padding token \u4e0d\u8ba1\u5165\uff09\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#bi-encoder","title":"\u4e3a\u4ec0\u4e48\u9002\u5408\u505a\u5411\u91cf\u68c0\u7d22\uff08Bi-Encoder\uff09","text":"<p>Sentence-Transformers \u5e38\u7528 Bi-Encoder\uff08\u53cc\u5854\uff09\uff1a - \u6587\u6863\u5757\u79bb\u7ebf\u7f16\u7801\u4e00\u6b21\u5f97\u5230\u5411\u91cf\uff08\u53ef\u7f13\u5b58/\u5165\u5e93\uff09 - \u67e5\u8be2\u65f6\u53ea\u7f16\u7801 query \u4e00\u6b21\uff0c\u518d\u505a\u8fd1\u90bb\u641c\u7d22</p> <p>\u8fd9\u6bd4 Cross-Encoder\uff08\u628a query+doc \u62fc\u5728\u4e00\u8d77\u9010\u4e2a\u6253\u5206\uff09\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u91cc\u66f4\u5feb\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#training-loss","title":"\u8bad\u7ec3\uff08Training\uff09\uff1a\u5e38\u89c1\u76ee\u6807\u4e0e Loss","text":"<p>\u8bad\u7ec3\u76ee\u6807\uff1a\u8ba9\u8bed\u4e49\u76f8\u8fd1\u7684\u53e5\u5b50\u5411\u91cf\u66f4\u8fd1\u3001\u8bed\u4e49\u4e0d\u76f8\u8fd1\u7684\u66f4\u8fdc\u3002\u5e38\u89c1\u6570\u636e\u5f62\u6001\uff1a - \u6210\u5bf9\uff08pair\uff09\uff1a(a, b, label) - \u4e09\u5143\u7ec4\uff08triplet\uff09\uff1a(anchor, positive, negative) - \u6392\u5e8f/\u68c0\u7d22\uff1aquery \u5bf9\u5e94\u591a\u4e2a relevant / irrelevant</p> <p>\u5e38\u89c1 loss\uff08\u4e0d\u540c\u6a21\u578b/\u8bad\u7ec3\u9636\u6bb5\u4f1a\u6df7\u7528\uff09\uff1a</p> <ul> <li>CosineSimilarityLoss\uff08\u62df\u5408\u76f8\u4f3c\u5ea6\u6807\u7b7e\uff09\uff1a</li> <li>\u76ee\u6807\uff1a\\(\\cos(u,v)\\) \u63a5\u8fd1\u6807\u6ce8\u7684\u76f8\u4f3c\u5ea6 \\(y\\)</li> <li>TripletLoss\uff08\u4e09\u5143\u7ec4\uff0c\u62c9\u8fd1\u6b63\u6837\u672c\u3001\u63a8\u8fdc\u8d1f\u6837\u672c\uff09\uff1a</li> </ul> \\[ \\max(0, \\text{margin} - \\text{sim}(a,p) + \\text{sim}(a,n)) \\] <ul> <li>MultipleNegativesRankingLoss\uff08\u68c0\u7d22\u5e38\u7528\uff09\uff1a</li> <li>\u4e00\u4e2a batch \u4e2d (a_i, p_i) \u662f\u6b63\u5bf9\uff0c\u5176\u4ed6 p_j \u81ea\u52a8\u5f53\u6210\u8d1f\u6837\u672c</li> <li>\u76ee\u6807\uff1a\u8ba9 a_i \u6700\u504f\u597d\u81ea\u5df1\u7684 p_i\uff08softmax \u5f52\u4e00\u5316\uff09</li> </ul> <p>\u8bf4\u660e\uff1a\u4f60\u9879\u76ee\u91cc\u7528\u5230\u7684 <code>all-MiniLM-L6-v2</code> \u662f\u5e38\u89c1\u7684\u8f7b\u91cf\u53e5\u5411\u91cf\u6a21\u578b\u914d\u7f6e\uff0c\u4f46\u201c\u7cbe\u786e\u5230\u8bad\u7ec3\u6570\u636e\u96c6/\u6b65\u9aa4\u201d\u7684\u7ec6\u8282\u9700\u67e5\u8be5\u6a21\u578b\u7684 model card \u624d\u80fd\u5b8c\u5168\u786e\u5b9a\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#9-hnsw","title":"9. HNSW\uff08\u5411\u91cf\u8fd1\u90bb\u68c0\u7d22\u7d22\u5f15\uff09","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#hnsw","title":"HNSW \u662f\u4ec0\u4e48\uff1f","text":"<p>HNSW = Hierarchical Navigable Small World\uff0c\u4e00\u79cd\u7528\u4e8e\u8fd1\u4f3c\u6700\u8fd1\u90bb\uff08ANN\uff09\u7684\u56fe\u7d22\u5f15\u7ed3\u6784\u3002 - \u5411\u91cf\u5e93\u5982\u679c\u201c\u66b4\u529b\u201d\u7b97\u6700\u8fd1\u90bb\uff0c\u9700\u8981\u5bf9\u6bcf\u4e2a\u5411\u91cf\u90fd\u7b97\u8ddd\u79bb\uff0c\u590d\u6742\u5ea6\u7ea6 \\(O(N)\\)\uff08N \u5f88\u5927\u65f6\u5f88\u6162\uff09 - HNSW \u7528\u56fe\u7ed3\u6784\u628a\u641c\u7d22\u52a0\u901f\u5230\u201c\u5c11\u91cf\u8282\u70b9\u7684\u8ddd\u79bb\u8ba1\u7b97\u201d\uff0c\u5b9e\u73b0\u9ad8\u6548 top-k \u68c0\u7d22</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_12","title":"\u76f4\u89c9\uff1a\u591a\u5c42\u56fe + \u8d2a\u5fc3\u641c\u7d22","text":"<p>\u53ef\u4ee5\u628a\u5b83\u60f3\u6210\u201c\u591a\u5c42\u9ad8\u901f\u8def/\u57ce\u5e02\u9053\u8def\u201d\uff1a - \u9ad8\u5c42\uff1a\u8282\u70b9\u66f4\u7a00\u758f\uff0c\u80fd\u5feb\u901f\u8df3\u8fdc\u5b9a\u4f4d\u5927\u81f4\u533a\u57df - \u4f4e\u5c42\uff1a\u8282\u70b9\u66f4\u5bc6\u96c6\uff0c\u5728\u5c40\u90e8\u505a\u7cbe\u7ec6\u641c\u7d22</p> <p>\u5178\u578b\u67e5\u8be2\u8fc7\u7a0b\uff1a 1. \u4ece\u6700\u9ad8\u5c42\u5165\u53e3\u70b9\u5f00\u59cb 2. \u5728\u8be5\u5c42\u505a\u8d2a\u5fc3\u8df3\u8f6c\uff1a\u5982\u679c\u67d0\u4e2a\u90bb\u5c45\u66f4\u63a5\u8fd1 query\uff0c\u5c31\u79fb\u52a8\u8fc7\u53bb 3. \u4e00\u5c42\u5c42\u5f80\u4e0b\uff0c\u9010\u6e10\u7cbe\u7ec6\u5316 4. \u5728\u6700\u5e95\u5c42\u7ef4\u62a4\u5019\u9009\u96c6\u5408\uff0c\u8fd4\u56de top-k</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_13","title":"\u4e3a\u4ec0\u4e48\u9002\u5408\u6587\u672c\u5411\u91cf\u68c0\u7d22","text":"<p>\u6587\u672c embedding \u7ef4\u5ea6\u9ad8\uff08\u5982 384 \u7ef4\uff09\uff0c\u7cbe\u786e\u6700\u8fd1\u90bb\u4ee3\u4ef7\u5927\u3002 HNSW \u5728\u9ad8\u7ef4\u7a7a\u95f4\u91cc\u901a\u5e38\u80fd\u63d0\u4f9b\u5f88\u597d\u7684\u901f\u5ea6/\u53ec\u56de\u6298\u4e2d\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#hnsw_1","title":"HNSW \u4e0e\u201c\u8ddd\u79bb\u5ea6\u91cf\u201d\u7684\u5173\u7cfb","text":"<p>HNSW \u672c\u8eab\u662f\u7d22\u5f15/\u641c\u7d22\u7ed3\u6784\uff1b\u201c\u76f8\u4f3c/\u8ddd\u79bb\u600e\u4e48\u8ba1\u7b97\u201d\u7531\u5ea6\u91cf\u51b3\u5b9a\u3002 \u672c\u9879\u76ee\u5728\u521b\u5efa collection \u65f6\u6307\u5b9a\uff1a</p> <ul> <li><code>metadata={\"hnsw:space\": \"cosine\"}</code>\uff08\u89c1 mcp_server/rag_engine.py\uff09</li> </ul> <p>\u8fd9\u610f\u5473\u7740 ChromaDB \u5728 HNSW \u91cc\u7528 cosine \u7a7a\u95f4\u8fdb\u884c\u8ddd\u79bb\u8ba1\u7b97\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_14","title":"\u5173\u952e\u53c2\u6570\uff08\u7406\u89e3\u5c42\u9762\uff09","text":"<p>\u4e0d\u540c\u5b9e\u73b0/\u6570\u636e\u5e93\u547d\u540d\u7565\u6709\u5dee\u5f02\uff0c\u4f46 HNSW \u5e38\u89c1\u6709\u8fd9\u4e9b\u63a7\u5236\u70b9\uff1a - M\uff1a\u6bcf\u4e2a\u8282\u70b9\u4fdd\u7559\u7684\u90bb\u5c45\u6570\uff08\u56fe\u66f4\u5bc6\u2192\u53ec\u56de\u66f4\u597d\u4f46\u5185\u5b58\u66f4\u5927\uff09 - ef_construction\uff1a\u5efa\u56fe\u65f6\u7684\u5019\u9009\u5bbd\u5ea6\uff08\u8d8a\u5927\u7d22\u5f15\u8d28\u91cf\u8d8a\u597d\u4f46\u6784\u5efa\u66f4\u6162\uff09 - ef_search\uff1a\u67e5\u8be2\u65f6\u7684\u5019\u9009\u5bbd\u5ea6\uff08\u8d8a\u5927\u53ec\u56de\u8d8a\u597d\u4f46\u67e5\u8be2\u66f4\u6162\uff09</p> <p>\u4f60\u5728\u672c\u4ed3\u5e93\u91cc\u6ca1\u6709\u663e\u5f0f\u8bbe\u7f6e\u8fd9\u4e9b\u53c2\u6570\uff08\u53ea\u8bbe\u7f6e\u4e86 <code>hnsw:space</code>\uff09\uff0c\u8bf4\u660e\u4f7f\u7528\u7684\u662f ChromaDB \u9ed8\u8ba4\u503c\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_15","title":"\u5728\u672c\u9879\u76ee\u7684\u5177\u4f53\u843d\u70b9","text":"<p>\u5f53\u4f60\u8c03\u7528\uff1a</p> <ul> <li><code>collection.query(query_embeddings=[...], n_results=k, ...)</code></li> </ul> <p>ChromaDB \u4f1a\uff1a 1. \u4f7f\u7528 HNSW \u7d22\u5f15\u5feb\u901f\u627e\u5230\u8fd1\u90bb\u5019\u9009 2. \u8ba1\u7b97 cosine \u8ddd\u79bb\u5e76\u6392\u5e8f 3. \u8fd4\u56de <code>ids/documents/metadatas/distances</code></p> <p>\u7136\u540e\u5e94\u7528\u5c42\u628a <code>distance</code> \u8f6c\u6210 <code>similarity = 1 - distance</code>\uff08\u89c1 mcp_server/rag_engine.py\uff09\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#10-logging_utilspy","title":"10. logging_utils.py\uff08\u6570\u636e\u5e93\u65e5\u5fd7\u4e0e\u7edf\u8ba1\u5de5\u5177\u5c42\uff09","text":"","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_16","title":"\u5b83\u7684\u5b9a\u4f4d","text":"<p><code>mcp_server/logging_utils.py</code> \u4e0d\u662f\u201c\u53ea\u6709\u67e5\u8be2\u6570\u636e\u5e93\u201d\uff0c\u66f4\u51c6\u786e\u662f\uff1a - \u5199\u6570\u636e\u5e93\uff08logging\uff09\uff1a\u8bb0\u5f55 MCP Server \u7684\u8fd0\u884c\u4e8b\u4ef6\uff08\u67e5\u8be2\u3001\u7d22\u5f15\u4efb\u52a1\u3001\u6307\u6807\uff09 - \u8bfb\u6570\u636e\u5e93\uff08stats \u805a\u5408\uff09\uff1a\u63d0\u4f9b\u4e00\u4e9b\u6c47\u603b\u7edf\u8ba1\u7ed9 MCP \u8d44\u6e90\uff08<code>stats://...</code>\uff09 - \u975e\u6570\u636e\u5e93\uff1a\u4e5f\u5305\u542b\u5373\u65f6\u91c7\u6837\u7684\u7cfb\u7edf\u72b6\u6001\uff08psutil\uff09\u548c\u8ba1\u65f6\u5de5\u5177</p> <p>\u5b83\u4f9d\u8d56\u7684\u8868\u7ed3\u6784\u6765\u81ea mcp_server/models.py\u3002</p> <p>\u8865\u5145\uff1a<code>mcp_server/models.py</code> \u672c\u8eab\u5c31\u662f PostgreSQL schema \u5b9a\u4e49\uff0c\u5728\u4ee3\u7801\u91cc\u5b9a\u4e49\u4e86\u4e09\u5f20\u8868\uff1a - <code>query_logs</code> - <code>indexing_jobs</code> - <code>system_metrics</code></p> <p>\u5e76\u901a\u8fc7 <code>init_db()</code> \u5728\u542f\u52a8\u65f6\u521b\u5efa\u8fd9\u4e9b\u8868\uff08\u4e0d\u5b58\u5728\u624d\u521b\u5efa\uff09\u3002</p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#postgresql","title":"\u5199\u6570\u636e\u5e93\uff1a\u628a\u8fd0\u884c\u4e8b\u4ef6\u843d\u5230 PostgreSQL","text":"<ul> <li><code>log_query(...)</code> \u2192 \u63d2\u5165\u4e00\u6761 <code>query_logs</code></li> <li>\u7531 MCP \u5de5\u5177 <code>query_documents</code> / <code>search_similar</code> \u8c03\u7528\uff08\u89c1 mcp_server/server.py\uff09</li> <li> <p>\u4fdd\u5b58\uff1aquery_text\u3001response_text\u3001sources(JSONB)\u3001response_time_ms\u3001client_type\u3001session_id</p> </li> <li> <p><code>create_indexing_job(...)</code> \u2192 \u63d2\u5165\u4e00\u6761 <code>indexing_jobs</code></p> </li> <li> <p>\u7d22\u5f15\u5f00\u59cb\u524d\u521b\u5efa job\uff0c\u521d\u59cb <code>status=\"pending\"</code></p> </li> <li> <p><code>update_indexing_job(job_id, ...)</code> \u2192 \u66f4\u65b0 <code>indexing_jobs</code></p> </li> <li><code>status=\"processing\"</code> \u4e14 started_at \u4e3a\u7a7a\u65f6\u5199\u5165 started_at</li> <li><code>status in (\"completed\",\"failed\")</code> \u65f6\u5199\u5165 completed_at</li> <li> <p>\u540c\u65f6\u66f4\u65b0 processed/failed/total/progress/current_file/error_message \u7b49</p> </li> <li> <p><code>log_system_metrics()</code> \u2192 \u63d2\u5165\u591a\u6761 <code>system_metrics</code></p> </li> <li>\u4f7f\u7528 psutil \u91c7\u6837 CPU/\u5185\u5b58/\u78c1\u76d8\u767e\u5206\u6bd4</li> </ul> <p>\u7edf\u4e00\u6a21\u5f0f\uff1a <pre><code>async with async_session() as session:\n    session.add(...)\n    await session.commit()\n</code></pre></p>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#mcp-stats","title":"\u8bfb\u6570\u636e\u5e93\uff1a\u805a\u5408\u7edf\u8ba1\uff08MCP stats:// \u8d44\u6e90\u7528\uff09","text":"<ul> <li><code>get_query_stats()</code></li> <li>total count\uff08\u603b\u67e5\u8be2\u6570\uff09</li> <li>avg response_time_ms\uff08\u5e73\u5747\u8017\u65f6\uff09</li> <li> <p>\u6700\u8fd1 10 \u6761 queries\uff08\u7528\u4e8e\u5feb\u901f\u5c55\u793a\uff09</p> </li> <li> <p><code>get_job_stats()</code></p> </li> <li>\u603b job \u6570</li> <li>\u6309 status \u8ba1\u6570\uff08pending/processing/completed/failed\uff09</li> <li>\u6d3b\u8dc3 job \u5217\u8868\uff08pending + processing\uff09</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_17","title":"\u975e\u6570\u636e\u5e93\uff1a\u5373\u65f6\u7cfb\u7edf\u72b6\u6001\u4e0e\u8ba1\u65f6","text":"<ul> <li><code>get_system_stats()</code>\uff1a\u4e0d\u8bfb DB\uff0c\u76f4\u63a5 psutil \u91c7\u6837\u5e76\u8fd4\u56de dict\uff08\u66f4\u50cf\u5373\u65f6\u5065\u5eb7\u5feb\u7167\uff09</li> <li><code>QueryTimer</code>\uff1a\u540c\u6b65\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u8ba1\u7b97\u8017\u65f6\u6beb\u79d2\u6570</li> </ul>","tags":[]},{"location":"2026/2026-02/2026-02-23-epstein-rag-learning-notes-4dca58fc/#_18","title":"\u4e0b\u4e00\u6b65\u5b66\u4e60\u65b9\u5411","text":"<p>\u63a8\u8350\u6309\u4ee5\u4e0b\u987a\u5e8f\u6df1\u5165\u5b66\u4e60\uff1a</p> <ol> <li>\u2705 server.py - MCP \u670d\u52a1\u7ed3\u6784\uff08\u5df2\u5b8c\u6210\uff09</li> <li>\u2705 rag_engine.py - RAG \u6838\u5fc3\u903b\u8f91 + ChromaDB\uff08\u5df2\u5b8c\u6210\uff09</li> <li>\u23ed\ufe0f logging_utils.py &amp; models.py - \u6570\u636e\u843d\u5e93</li> <li>dashboard_backend/api/queries.py - \u7edf\u8ba1\u63a5\u53e3</li> <li>services/pipeline.py - \u79bb\u7ebf\u5efa\u5e93\u6d41\u7a0b</li> </ol>","tags":[]},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/","title":"LoRA Fine-Tuning Complete Guide","text":"<p>Learned from fine-tuning Qwen3-0.6B with ruozhiba dataset using LLaMA-Factory.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>LoRA Algorithm</li> <li>QLoRA (Quantized LoRA)</li> <li>Training Parameters</li> <li>Batch Size, Gradient Accumulation &amp; Cutoff Length</li> <li>Output &amp; Logging Parameters</li> <li>Training Loss vs Eval Loss Curves</li> <li>Parameter Update Calculations</li> <li>Experiment Results</li> </ol>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#1-lora-algorithm","title":"1. LoRA Algorithm","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#the-problem","title":"The Problem","text":"<p>A model like Qwen3-0.6B has 600 million parameters. Full fine-tuning means updating all 600M parameters - expensive in GPU memory, storage, and time.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#the-core-idea","title":"The Core Idea","text":"<p>LoRA's insight: you don't need to change all parameters. The changes during fine-tuning can be represented by much smaller matrices.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#how-it-works","title":"How It Works","text":"<p>Original weight matrix W (e.g., 1024 x 1024 = 1,048,576 params):</p> <pre><code>Input \u2192 [W] \u2192 Output\n</code></pre> <p>With LoRA, freeze W and add two small matrices A and B:</p> <pre><code>Input \u2192 [W (frozen)] + [A \u00d7 B (trainable)] \u2192 Output\n</code></pre> <p>Where:</p> <ul> <li>A is 1024 x r (r = rank, e.g., 8)</li> <li>B is r x 1024</li> </ul> <p>Trainable params: 1024\u00d78 + 8\u00d71024 = 16,384 (vs 1,048,576)</p> <p>That's 98.4% fewer parameters!</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#key-parameters","title":"Key Parameters","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#rank-r-the-bottleneck-dimension","title":"rank (r) - The bottleneck dimension","text":"<pre><code>r=4  \u2192 Very compressed, fast, may underfit\nr=8  \u2192 Standard, good balance\nr=16 \u2192 More capacity\nr=32 \u2192 High capacity\nr=64 \u2192 Near full fine-tuning quality\n</code></pre> <p>Experiment results on Qwen3-0.6B with ruozhiba dataset:</p> <ul> <li>rank=8:  Train Loss 1.967</li> <li>rank=16: Train Loss 1.905, Eval Loss 1.866</li> <li>rank=32: Train Loss 1.827, Eval Loss 1.854 (BEST)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#alpha-scaling-factor","title":"alpha - Scaling factor","text":"<pre><code>Actual update = (alpha / rank) \u00d7 A \u00d7 B\n</code></pre> <p>Rule of thumb: alpha = 2 \u00d7 rank</p> <p>Examples:</p> <ul> <li>alpha=16, rank=8 \u2192 scale = 2.0</li> <li>alpha=64, rank=32 \u2192 scale = 2.0</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#dropout-regularization","title":"dropout - Regularization","text":"<pre><code>dropout=0.0  \u2192 No regularization\ndropout=0.05 \u2192 Light (standard)\ndropout=0.1  \u2192 Heavy (can hurt performance)\n</code></pre> <p>Experiment showed dropout=0.1 performed WORST (Eval Loss 1.883 vs 1.854 for best).</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#target-modules-which-layers-to-adapt","title":"target modules - Which layers to adapt","text":"<p>lora_target=all applies LoRA to:</p> <ul> <li>q_proj (query) - Attention</li> <li>k_proj (key) - Attention</li> <li>v_proj (value) - Attention</li> <li>o_proj (output) - Attention</li> <li>gate_proj - MLP</li> <li>up_proj - MLP</li> <li>down_proj - MLP</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#comparison-full-fine-tuning-vs-lora","title":"Comparison: Full Fine-Tuning vs LoRA","text":"Aspect Full Fine-Tuning LoRA Params trained 600M (100%) ~10-40M (2-7%) GPU memory High Low Storage per model ~1.2 GB ~20 MB adapter Quality Best Very close Speed Slow Fast Multiple tasks Save full model Swap adapters"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#lora-variants","title":"LoRA Variants","text":"<ul> <li>QLoRA: LoRA + 4-bit quantization (saves even more memory)</li> <li>DoRA: Weight-decomposed LoRA (newer, sometimes better)</li> <li>rsLoRA: Rank-stabilized LoRA (scales better with high rank)</li> <li>PiSSA: Principal Singular values and Singular vectors Adaptation</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#practical-notes","title":"Practical Notes","text":"<ul> <li>LoRA's biggest advantage: keep one base model and swap tiny adapter files for different tasks</li> <li>4 adapters \u2248 80 MB total vs ~5 GB for 4 full fine-tuned models</li> <li>Always use validation data to detect overfitting</li> <li>Best checkpoint is often NOT the final one - use early stopping</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#2-qlora-quantized-lora","title":"2. QLoRA (Quantized LoRA)","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#the-problem-lora-didnt-solve","title":"The Problem LoRA Didn't Solve","text":"<p>LoRA only trains small adapter matrices (~20MB), but the frozen base model still sits in GPU memory at full precision. For large models this is still too much:</p> <ul> <li>Qwen3-7B in bf16: ~14 GB GPU memory</li> <li>Qwen3-72B in bf16: ~144 GB GPU memory</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#qloras-solution","title":"QLoRA's Solution","text":"<p>Quantize the frozen base model to 4-bit, then apply LoRA adapters on top at full precision.</p> <pre><code>LoRA:   Base model (bf16, 16-bit) + LoRA adapters (small)\nQLoRA:  Base model (4-bit)        + LoRA adapters (bf16, full precision)\n</code></pre> <p>The base model is 4x smaller in memory. Only the LoRA adapters (which are being trained) stay at full precision.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#memory-comparison","title":"Memory Comparison","text":"Model Size Full Fine-Tune LoRA (bf16) QLoRA (4-bit) 0.6B ~2.4 GB ~1.2 GB ~0.4 GB 7B ~28 GB ~14 GB ~4 GB 14B ~56 GB ~28 GB ~8 GB 72B ~288 GB ~144 GB ~36 GB <p>QLoRA makes 7B models trainable on a single 8GB consumer GPU.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#how-4-bit-quantization-works","title":"How 4-bit Quantization Works","text":"<p>Normal bf16: Each weight stored as 16-bit float (65,536 possible values) 4-bit NF4: Each weight mapped to nearest of 16 values (2^4 = 16)</p> <p>Information is lost, but frozen weights don't need full precision. Only the LoRA adapters need full precision for gradient computation.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#qlora-key-techniques","title":"QLoRA Key Techniques","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#1-nf4-normalfloat-4-bit","title":"1. NF4 (NormalFloat 4-bit)","text":"<p>Quantization format optimized for neural network weight distributions (normally distributed). Less information loss than naive 4-bit.</p> <pre><code>quantization_type: nf4\n</code></pre>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#2-double-quantization","title":"2. Double Quantization","text":"<p>Quantizes the quantization constants themselves. Saves extra ~0.4 bits per parameter.</p> <pre><code>double_quantization: true\n</code></pre>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#3-paged-optimizers","title":"3. Paged Optimizers","text":"<p>Uses CPU RAM as overflow when GPU runs out. Prevents OOM on long sequences.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#how-to-use-qlora-in-llama-factory","title":"How to Use QLoRA in LLaMA-Factory","text":"<p>Add these lines to your training config:</p> <pre><code>quantization_bit: 4              # 4-bit quantization\nquantization_method: bnb         # Use bitsandbytes library\nquantization_type: nf4           # NormalFloat4 (best)\ndouble_quantization: true        # Quantize the quantization constants\n</code></pre> <p>Everything else (lora_rank, learning_rate, epochs, etc.) stays the same as regular LoRA.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#lora-vs-qlora-comparison","title":"LoRA vs QLoRA Comparison","text":"Aspect LoRA QLoRA Base model precision bf16 (16-bit) 4-bit GPU memory Higher ~4x less Training speed Faster Slower (dequantize) Quality Best ~97-99% of LoRA When to use GPU has enough GPU too small"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#when-you-need-qlora","title":"When You Need QLoRA","text":"<ul> <li>0.6B model: LoRA is fine, model is tiny</li> <li>7B model on 8GB GPU: NEED QLoRA</li> <li>14B model on 24GB GPU: NEED QLoRA</li> <li>72B model: NEED QLoRA + multi-GPU</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#quality-impact","title":"Quality Impact","text":"<ul> <li>Full fine-tune: 100% quality (reference)</li> <li>LoRA: ~99% quality</li> <li>QLoRA: ~97-99% quality</li> </ul> <p>The quality difference is usually negligible in practice.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#3-training-parameters","title":"3. Training Parameters","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#learning-depth-parameters","title":"Learning Depth Parameters (\u63a7\u5236\u5b66\u4e60\u6df1\u5ea6)","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#lora_rank","title":"lora_rank","text":"<p>The bottleneck dimension of LoRA matrices. Controls model capacity.</p> <ul> <li>r=8: Standard, good balance (baseline)</li> <li>r=16: More capacity</li> <li>r=32: High capacity (best in experiments)</li> <li>r=64: Near full fine-tuning quality</li> <li>Higher rank = more trainable parameters = better fit but more memory</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#learning_rate","title":"learning_rate","text":"<p>How big each update step is. Step size when walking downhill to find the lowest loss.</p> <ul> <li>1e-3: Too high, overshoots</li> <li>5e-5: Standard for LoRA fine-tuning</li> <li>1e-5: Conservative, more stable but slower</li> <li>1e-6: Too low, may get stuck</li> <li>Rule of thumb for LoRA: 1e-5 to 5e-5</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#lr_scheduler_type","title":"lr_scheduler_type","text":"<p>How learning rate changes over training time.</p> <ul> <li>cosine: Starts high, smoothly decreases like cosine curve (most popular)</li> <li>linear: Decreases at constant rate</li> <li>constant: Never changes</li> <li>Cosine lets model learn fast early, then fine-tune details later</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#warmup_ratio","title":"warmup_ratio","text":"<p>Gradually increase LR at the start instead of jumping to full speed.</p> <ul> <li>0.1 = first 10% of steps are warmup</li> <li>Prevents instability from large LR on random/early weights</li> <li>Training flow: 0 \u2192 peak LR (warmup) \u2192 cosine decay \u2192 ~0</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#max_samples","title":"max_samples","text":"<p>Limit how many samples from the dataset to use.</p> <ul> <li>Useful for quick experiments and debugging</li> <li>Example: 1500 means only use 1500 samples even if dataset has more</li> <li>Omit to use full dataset</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#num_train_epochs","title":"num_train_epochs","text":"<p>How many times to pass through the entire dataset.</p> <ul> <li>epoch=1: See each sample once (underfitting risk)</li> <li>epoch=3: Standard (good balance)</li> <li>epoch=5: More training (overfitting risk)</li> <li>More epochs = more training but risk of overfitting</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#evaluation-parameters","title":"Evaluation Parameters (\u63a7\u5236\u6d4b\u8bd5\u96c6)","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#val_size","title":"val_size","text":"<p>Split portion of training data for validation.</p> <ul> <li>val_size: 0.1 means 10% of training data becomes validation set</li> <li>Range: 0.0 to 1.0 (float) or integer for exact count</li> <li>Used to detect overfitting during training</li> <li>If train_loss goes down but eval_loss goes up = overfitting</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#eval_dataset","title":"eval_dataset","text":"<p>Specify a separate dataset for evaluation instead of splitting training data.</p> <ul> <li>Alternative to val_size</li> <li>Points to a dataset name defined in dataset_info.json</li> <li>Better than val_size because train and eval data are completely separate</li> <li>Example: eval_dataset: ruozhiba_test</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#per_device_eval_batch_size","title":"per_device_eval_batch_size","text":"<p>How many samples to evaluate at once per GPU.</p> <ul> <li>Higher = faster evaluation but more GPU memory</li> <li>Typical values: 1, 2, 4, 8</li> <li>Does NOT affect training quality, only evaluation speed</li> <li>Can be larger than training batch size (no gradients needed)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#eval_strategy","title":"eval_strategy","text":"<p>When to run evaluation.</p> <ul> <li>steps: Evaluate every N steps (use with eval_steps)</li> <li>epoch: Evaluate at end of each epoch</li> <li>no: Never evaluate (default)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#eval_steps","title":"eval_steps","text":"<p>How often to evaluate (when eval_strategy=steps).</p> <ul> <li>eval_steps: 50 means evaluate every 50 training steps</li> <li>Lower = more frequent evaluation = slower training but better monitoring</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#load_best_model_at_end","title":"load_best_model_at_end","text":"<p>After training, load the checkpoint with best eval_loss.</p> <ul> <li>Requires eval_strategy to be set</li> <li>Saves the best model, not just the final one</li> <li>Critical for preventing overfitting</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#metric_for_best_model","title":"metric_for_best_model","text":"<p>Which metric determines the \"best\" checkpoint.</p> <ul> <li>eval_loss: Most common, lower is better</li> <li>eval_accuracy: If compute_accuracy is enabled</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#4-batch-size-gradient-accumulation-cutoff-length","title":"4. Batch Size, Gradient Accumulation &amp; Cutoff Length","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#cutoff_len-cutoff_length","title":"cutoff_len (cutoff_length)","text":"<p>Maximum token length for each training sample.</p> <pre><code>cutoff_len: 2048\n</code></pre> <p>What happens to samples based on their token count:</p> <ul> <li>Tokens &lt; cutoff_len: Padded with pad tokens to fill</li> <li>Tokens = cutoff_len: Used as-is</li> <li>Tokens &gt; cutoff_len: TRUNCATED (tail cut off, data lost!)</li> </ul> <p>Choosing the right value:</p> <ul> <li>512: Fast, low memory, but long samples get cut</li> <li>1024: Good for short Q&amp;A datasets</li> <li>2048: Safe for most datasets (standard)</li> <li>4096: Long conversations, requires more GPU memory</li> </ul> <p>Trade-off: Higher = more GPU memory per sample but preserves long text. Lower = faster training, less memory, but loses long content.</p> <p>For short Q&amp;A data like ruozhiba (~30-100 tokens per sample), 2048 is overkill but safe.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#per_device_train_batch_size","title":"per_device_train_batch_size","text":"<p>How many samples each GPU processes in one forward pass.</p> <pre><code>per_device_train_batch_size: 4\n</code></pre> <p>Effects of batch size:</p> <ul> <li>Larger batch: Smoother gradients, faster training, but more GPU memory</li> <li>Smaller batch: Less GPU memory, can generalize better (noise acts as regularization), but noisier and slower</li> </ul> <p>If you get OOM (Out of Memory), reduce batch size first, then compensate with gradient_accumulation_steps.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<p>Simulate a larger batch size without needing more GPU memory.</p> <pre><code>gradient_accumulation_steps: 4\n</code></pre> <p>How it works:</p> <ul> <li>Normal: Forward N samples \u2192 Backward \u2192 UPDATE weights immediately</li> <li>With accumulation=4: Forward N samples \u2192 save gradient, repeat 4 times \u2192 AVERAGE all gradients \u2192 UPDATE weights once</li> </ul> <p>This means the model sees 4x more samples before each weight update, but only holds 1 mini-batch in GPU memory at a time.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#effective-batch-size-formula","title":"Effective Batch Size Formula","text":"<pre><code>effective_batch = per_device_batch \u00d7 grad_accum \u00d7 num_GPUs\n</code></pre> <p>Examples from experiments:</p> <ul> <li>Baseline: 4 \u00d7 4 \u00d7 2 GPUs = 32 effective batch</li> <li>Exp4:     2 \u00d7 8 \u00d7 2 GPUs = 32 effective batch (same result, less memory!)</li> </ul> <p>Same effective batch size, different memory usage:</p> <ul> <li>batch=8, accum=1: 8 samples in GPU at once (high memory)</li> <li>batch=4, accum=2: 4 samples in GPU at once (medium memory)</li> <li>batch=2, accum=4: 2 samples in GPU at once (low memory)</li> <li>batch=1, accum=8: 1 sample in GPU at once (minimum memory)</li> </ul> <p>All produce identical training behavior but use different amounts of GPU memory. This is the key trick for training large models on limited hardware.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#how-these-three-interact","title":"How These Three Interact","text":"<pre><code>Total training steps = (num_samples / effective_batch) \u00d7 num_epochs\n</code></pre> <p>Example with ruozhiba (5,986 samples), effective_batch=32, 3 epochs:</p> <ul> <li>Steps = (5986 / 32) \u00d7 3 \u2248 141 steps (matches actual training!)</li> </ul> <p>If cutoff_len is too high and samples are long:</p> <ul> <li>Each sample uses more GPU memory</li> <li>May need to reduce batch_size</li> <li>Compensate with higher gradient_accumulation_steps</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#practical-guidelines","title":"Practical Guidelines","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#cutoff_len","title":"cutoff_len","text":"<ul> <li>Check your dataset: what's the max token length?</li> <li>Set cutoff_len slightly above the 95th percentile</li> <li>Don't set unnecessarily high (wastes memory on padding)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#batch_size","title":"batch_size","text":"<ul> <li>Start with 4, increase if GPU memory allows</li> <li>Reduce to 2 or 1 if OOM errors occur</li> <li>Typical range: 1-8 for LoRA fine-tuning</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#gradient_accumulation","title":"gradient_accumulation","text":"<ul> <li>Use to reach target effective batch size (16-64 is common)</li> <li>Higher accumulation = fewer weight updates per epoch</li> <li>No extra GPU memory cost, only slightly slower (more forward passes)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#5-output-logging-parameters","title":"5. Output &amp; Logging Parameters","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#output_dir","title":"output_dir","text":"<p>Where all training outputs are saved. Everything goes into this directory.</p> <pre><code>output_dir: saves/qwen3-0.6b/lora/sft\n</code></pre> <p>Contents of output_dir after training:</p> <pre><code>saves/qwen3-0.6b/lora/sft/\n\u251c\u2500\u2500 adapter_model.safetensors    \u2190 LoRA weights (final model)\n\u251c\u2500\u2500 adapter_config.json          \u2190 LoRA configuration\n\u251c\u2500\u2500 checkpoint-100/              \u2190 Intermediate checkpoint at step 100\n\u251c\u2500\u2500 checkpoint-141/              \u2190 Checkpoint at step 141\n\u251c\u2500\u2500 trainer_state.json           \u2190 Training logs, metrics, loss history\n\u251c\u2500\u2500 training_loss.png            \u2190 Loss curve visualization\n\u251c\u2500\u2500 train_results.json           \u2190 Final summary metrics\n\u251c\u2500\u2500 tokenizer.json               \u2190 Tokenizer files\n\u251c\u2500\u2500 tokenizer_config.json        \u2190 Tokenizer configuration\n\u2514\u2500\u2500 all_results.json             \u2190 Combined results\n</code></pre> <p>Best practice: use different output_dir for each experiment to keep results separate.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#overwrite_output_dir","title":"overwrite_output_dir","text":"<p>Controls what happens if output_dir already exists from a previous run.</p> <pre><code>overwrite_output_dir: true    # Delete old files, start fresh\noverwrite_output_dir: false   # ERROR if directory exists (default, safe)\n</code></pre> <ul> <li>true: Convenient for re-running experiments, but LOSES previous results</li> <li>false: Safe, prevents accidental deletion of good checkpoints</li> <li>Default is false for safety</li> </ul> <p>Use true during experimentation, false for important/final training runs.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#logging_steps","title":"logging_steps","text":"<p>How often to print and record training metrics during training.</p> <pre><code>logging_steps: 10    # Log every 10 training steps\n</code></pre> <p>Each log entry records: loss, learning_rate, grad_norm, epoch, step.</p> <p>Example output during training:</p> <pre><code>Step 10:  loss=2.677, lr=3.0e-05, grad_norm=2.05\nStep 20:  loss=2.337, lr=4.9e-05, grad_norm=1.49\nStep 30:  loss=2.118, lr=4.8e-05, grad_norm=1.33\n</code></pre> <p>Different values and their effects:</p> <ul> <li>logging_steps: 1   \u2192 Every step, very verbose, slightly slower</li> <li>logging_steps: 10  \u2192 Every 10 steps, good balance (standard)</li> <li>logging_steps: 50  \u2192 Every 50 steps, less detail</li> <li>logging_steps: 100 \u2192 Minimal logging, fast but may miss trends</li> </ul> <p>Trade-off: Lower = more detailed loss curve but slightly slower. Higher = less noise and faster but may miss important trends.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#related-parameters","title":"Related Parameters","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#save_steps","title":"save_steps","text":"<p>How often to save a checkpoint (model weights snapshot).</p> <pre><code>save_steps: 100    # Save checkpoint every 100 steps\n</code></pre> <p>Creates checkpoint-100/, checkpoint-200/, etc. in output_dir.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#save_total_limit","title":"save_total_limit","text":"<p>Maximum number of checkpoints to keep. Older ones are deleted.</p> <pre><code>save_total_limit: 3    # Keep only 3 most recent checkpoints\n</code></pre> <p>Prevents disk space from filling up during long training runs.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#plot_loss","title":"plot_loss","text":"<p>Whether to generate a loss curve PNG image after training.</p> <pre><code>plot_loss: true    # Generates training_loss.png in output_dir\n</code></pre>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#6-training-loss-vs-eval-loss-curves","title":"6. Training Loss vs Eval Loss Curves","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#two-types-of-loss-charts","title":"Two Types of Loss Charts","text":"<p>When you set <code>plot_loss: true</code> and have validation enabled, LLaMA-Factory generates two PNG files:</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#1-training_losspng-training-loss","title":"1. training_loss.png - Training Loss","text":"<p>Tracks loss on training data over all steps.</p> <ul> <li>Shows how well the model fits the data it has seen</li> <li>Y-axis: loss (training), X-axis: training step</li> <li>Generated for every training run</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#2-training_eval_losspng-eval-loss","title":"2. training_eval_loss.png - Eval Loss","text":"<p>Tracks loss on validation data (held-out portion).</p> <ul> <li>Shows how well the model generalizes to unseen data</li> <li>Y-axis: eval_loss, X-axis: step</li> <li>Only generated when eval_strategy is set (steps or epoch)</li> <li>Number of data points depends on eval_steps</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#7-parameter-update-calculations","title":"7. Parameter Update Calculations","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#formula","title":"Formula","text":"<pre><code>total_updates (steps) = (num_samples / effective_batch) \u00d7 num_epochs\neffective_batch = per_device_batch \u00d7 grad_accum \u00d7 num_GPUs\n</code></pre>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#experiment-results-qwen3-06b-with-ruozhiba","title":"Experiment Results (Qwen3-0.6B with ruozhiba)","text":"Experiment Samples Eff. Batch Epochs Total Updates Baseline 5,986 32 3 141 Exp1 (r=16) 5,387 32 3 129 Exp2 (5 epochs) 5,387 32 5 215 Exp3 (more reg) 5,387 32 5 215 Exp4 (r=32) 5,387 32 3 129 <p>Note: Exp1-4 use 5,387 samples because val_size=0.1 holds out 10% (599 samples) for validation.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#what-happens-in-each-update","title":"What Happens in Each Update","text":"<p>1 update = 1 weight change = 1 \"step\"</p> <p>Each update:</p> <ol> <li>Forward: Feed effective_batch samples through model</li> <li>Loss: Calculate how wrong predictions are</li> <li>Backward: Compute gradients (direction to improve)</li> <li>Update: Adjust LoRA weights by (gradient \u00d7 learning_rate)</li> </ol>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#total-samples-seen","title":"Total Samples Seen","text":"<p>Each sample is seen num_epochs times:</p> Experiment Samples \u00d7 Epochs Total samples processed Baseline 5,986 \u00d7 3 17,958 Exp1 5,387 \u00d7 3 16,161 Exp2 5,387 \u00d7 5 26,935 Exp3 5,387 \u00d7 5 26,935 Exp4 5,387 \u00d7 3 16,161"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#key-insight","title":"Key Insight","text":"<p>Exp4 won with only 129 updates and 16,161 samples seen. Exp2/Exp3 used 67% more updates (215) but didn't beat it. Model capacity (LoRA rank) mattered more than training duration.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#effective-batch-size-trick","title":"Effective Batch Size Trick","text":"<p>Different configs can produce the same effective batch:</p> <ul> <li>batch=4, accum=4, 2 GPUs = 32 (baseline)</li> <li>batch=2, accum=8, 2 GPUs = 32 (exp4, less GPU memory)</li> <li>batch=1, accum=16, 2 GPUs = 32 (minimum memory)</li> </ul> <p>All produce identical training behavior but different memory usage.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#8-experiment-results","title":"8. Experiment Results","text":""},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#overview","title":"Overview","text":"<p>Conducted fine-tuning experiments on Qwen3-0.6B model using LLaMA-Factory with ruozhiba dataset (5,986 Chinese Q&amp;A pairs). Tested different hyperparameters to optimize model performance.</p>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#base-configuration","title":"Base Configuration","text":"<ul> <li>Model: Qwen3-0.6B (600M parameters)</li> <li>Method: LoRA (Low-Rank Adaptation)</li> <li>Dataset: ruozhiba (Chinese humor/QA)</li> <li>Format: Alpaca (instruction \u2192 output)</li> <li>Base Loss: 1.967 (original 3 epochs, rank 8)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#results","title":"Results","text":"Experiment rank epochs lr dropout Train Loss Eval Loss Baseline 8 3 5e-5 0.05 1.967 N/A Exp1 16 3 5e-5 0.05 1.905 1.866 Exp2 8 5 5e-5 0.05 1.885 1.859 Exp3 8 5 3e-5 0.10 1.958 1.883 Exp4 32 3 5e-5 0.05 1.827 1.854"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#key-findings","title":"Key Findings","text":"<ul> <li>Higher LoRA rank significantly improves performance (r=32 best)</li> <li>More epochs help but only to a point</li> <li>Early stopping crucial (best models not at final step)</li> <li>Too much regularization (dropout 0.1) hurts performance</li> <li>Validation data essential for detecting overfitting</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#best-model-exp4-high-capacity-r32","title":"Best Model: Exp4 High Capacity (r=32)","text":"<ul> <li>Eval Loss: 1.854 (5.7% improvement over baseline)</li> <li>Perplexity: 6.4</li> <li>Location: <code>saves/qwen3-0.6b/lora/exp4_high_capacity</code></li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#technical-details","title":"Technical Details","text":"<ul> <li>Framework: LLaMA-Factory</li> <li>Batch size: 4 (effective 32 with grad accumulation)</li> <li>Learning rate: 5e-5 (cosine with 10% warmup)</li> <li>Precision: bfloat16</li> <li>Training time: ~90-150s per experiment</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#data-formats","title":"Data Formats","text":"<ul> <li>Alpaca: instruction/input/output fields (used in this project)</li> <li>ShareGPT: messages array with roles (alternative for multi-turn chat)</li> </ul>"},{"location":"2026/2026-02/2026-02-24-lora-fine-tuning-complete-guide/#commands","title":"Commands","text":"<pre><code># Chat with best model\nllamafactory-cli chat \\\n  --model_name_or_path models/Qwen3-0.6B \\\n  --adapter_name_or_path saves/qwen3-0.6b/lora/exp4_high_capacity \\\n  --template qwen \\\n  --finetuning_type lora \\\n  --no_enable_thinking\n</code></pre>"},{"location":"2026/2026-02/2026-02-25-dashboard-backend-fastapi/","title":"Dashboard Backend (FastAPI) \u7b14\u8bb0","text":"<p>\u540e\u7aef\u4e3b\u8981\u7531 3 \u4e2a Python \u670d\u52a1 + 2 \u4e2a\u6570\u636e\u5b58\u50a8\u7ec4\u6210\uff0c\u56f4\u7ed5\u7d22\u5f15\u68c0\u7d22\u89c2\u6d4b\u95ed\u73af\u3002\u4e0b\u9762\u6309\u6a21\u5757\u8bb2\u6e05\u695a\uff0c\u5e76\u7ed9\u4f60\u8bfb\u4ee3\u7801\u7684\u5165\u53e3\u8def\u5f84\u3002</p> <p>\u6574\u4f53\u67b6\u6784</p> <p>\u6838\u5fc3 RAG \u5f15\u64ce\uff1amcp_server/\uff0c\u8d1f\u8d23\u7d22\u5f15\u548c\u67e5\u8be2 \u89c2\u6d4b API\uff1adashboard_backend/\uff0cFastAPI \u63d0\u4f9b\u67e5\u8be2/\u4efb\u52a1/\u5065\u5eb7/\u5206\u6790/WS \u6570\u636e\u7ba1\u7ebf\uff1aservices/\uff0c\u8d1f\u8d23\u6570\u636e\u4e0b\u8f7d\u3001\u89e3\u6790\u3001\u5206\u5757\u3001\u5199\u5165\u5411\u91cf\u5e93 \u6570\u636e\u5b58\u50a8\uff1aPostgreSQL\uff08\u65e5\u5fd7\u4e0e\u4efb\u52a1\uff09+ ChromaDB\uff08\u5411\u91cf\u5e93\uff09 \u7ec4\u5408\u542f\u52a8\uff1adocker-compose.yml\uff08\u670d\u52a1\u4f9d\u8d56\u4e0e\u7aef\u53e3\uff09</p> <p>1) MCP Server\uff08\u6838\u5fc3 RAG \u5f15\u64ce\uff09</p> <p>\u5165\u53e3\uff1aserver.py\uff0c\u901a\u8fc7 MCP stdio \u66b4\u9732 8 \u4e2a\u5de5\u5177\uff08index/query/search/delete/reset/status \u7b49\uff09 \u4e3b\u8981\u804c\u8d23\uff1a \u6587\u6863\u7d22\u5f15\u4e0e\u68c0\u7d22\uff1arag_engine.py \u8bfb\u53d6\u6587\u672c\u3001\u6309 chunk \u5207\u5206\u3001\u7528 sentence-transformers \u505a embedding \u5b58\u5230 ChromaDB\uff0c\u5e76\u80fd\u7528\u5411\u91cf\u76f8\u4f3c\u5ea6\u68c0\u7d22 \u7ed3\u6784\u5316\u65e5\u5fd7\uff1alogging_utils.py \u5c06\u67e5\u8be2\u65e5\u5fd7\u3001\u7d22\u5f15\u4efb\u52a1\u3001\u7cfb\u7edf\u6307\u6807\u5199\u5165 Postgres \u6570\u636e\u6a21\u578b\uff1amodels.py query_logs, indexing_jobs, system_metrics \u4e09\u5f20\u8868 \u914d\u7f6e\uff1aconfig.py\uff08DATABASE_URL/CHROMA_HOST/EMBEDDING_MODEL/CHUNK_SIZE \u7b49\uff09 2) Dashboard Backend\uff08\u89c2\u6d4b API + WebSocket\uff09</p> <p>\u5165\u53e3\uff1amain.py\uff0cFastAPI app + CORS + \u8def\u7531\u6ce8\u518c DB \u8bbf\u95ee\uff1adb.py\uff08Async SQLAlchemy\uff09 API \u8def\u7531\uff1a \u67e5\u8be2\u65e5\u5fd7\uff1aqueries.py /api/dashboard/queries \u5217\u8868\u4e0e\u8fc7\u6ee4 /api/dashboard/queries/stats \u805a\u5408\u7edf\u8ba1 \u4efb\u52a1\u76d1\u63a7\uff1ajobs.py \u5217\u8868\u3001\u8be6\u60c5\u3001\u8fdb\u5ea6\u3001\u53d6\u6d88 \u5065\u5eb7/\u6307\u6807/\u5206\u6790\uff1ahealth.py /health, /metrics, /analytics \u5411\u91cf\u641c\u7d22\uff1asearch.py \u76f4\u63a5\u8c03\u7528 ChromaDB \u641c\u7d22\u5e76\u8bb0\u5f55\u5230 query_logs WebSocket\uff1awebsocket.py /ws/dashboard \u63a8\u9001\u5b9e\u65f6\u66f4\u65b0\u4e0e\u5fc3\u8df3 \u6a21\u578b\u590d\u7528\uff1amodels.py \u4f1a\u4f18\u5148\u5bfc\u5165 mcp_server.models\uff0c\u786e\u4fdd\u8868\u7ed3\u6784\u4e00\u81f4 \u914d\u7f6e\uff1aconfig.py\uff08DASHBOARD_ \u524d\u7f00\u73af\u5883\u53d8\u91cf\uff09 3) Services\uff08\u6570\u636e\u7ba1\u7ebf\uff09</p> <p>\u603b\u7ba1\u7ebf\uff1apipeline.py \u4e0b\u8f7d\u6570\u636e  \u89e3\u6790/\u5206\u5757  \u5199\u5165 ChromaDB \u540c\u65f6\u66f4\u65b0 Postgres \u4e2d\u7684 indexing_jobs \u4e0b\u8f7d\u5668\uff1adataset_downloader.py GitHub \u514b\u9686\u6216 ZIP \u4e0b\u8f7d\uff0c\u5e26\u65ad\u70b9\u7eed\u4f20 \u6587\u6863\u5904\u7406\uff1adocument_processor.py PDF \u7528 PyMuPDF \u62bd\u53d6\u6587\u672c \u5206\u5757\u3001\u5143\u6570\u636e\u62bd\u53d6\u3001\u6587\u6863\u7c7b\u578b\u5206\u7c7b \u6570\u636e\u96c6\u51c6\u5907\uff1adata_prep.py \u4ece DOJ \u955c\u50cf\u4e0b\u8f7d zip\u3001\u89e3\u538b PDF\u3001\u518d\u8d70 pipeline \u5de5\u5177\u51fd\u6570\uff1autils.py \u6570\u636e\u6d41\uff08\u540e\u7aef\u6838\u5fc3\u8def\u5f84\uff09</p> <p>\u7d22\u5f15\uff1apipeline \u6216 MCP index_documents  \u89e3\u6790/\u5206\u5757  embedding  ChromaDB \u540c\u65f6\u5199\u5165 indexing_jobs\u3001\u65e5\u5fd7\u7b49\u5230 Postgres \u67e5\u8be2\uff1aMCP query_documents \u6216 Dashboard /search  ChromaDB \u5411\u91cf\u68c0\u7d22 \u7ed3\u679c + \u7edf\u8ba1\u5199\u5165 query_logs \u89c2\u6d4b\uff1aDashboard API \u8bfb\u53d6 Postgres\uff0c\u8ba1\u7b97\u8d8b\u52bf\u3001\u70ed\u529b\u56fe\u3001\u54cd\u5e94\u65f6\u95f4\u5206\u5e03\u7b49 \u5982\u679c\u4f60\u60f3\u66f4\u5feb\u4e0a\u624b\u9605\u8bfb\uff0c\u6211\u5efa\u8bae\u6309\u8fd9\u4e2a\u987a\u5e8f\u770b\uff1a</p> <p>server.py\uff08\u5de5\u5177\u5165\u53e3\u4e0e\u4e3b\u6d41\u7a0b\uff09 rag_engine.py\uff08\u7d22\u5f15\u4e0e\u68c0\u7d22\u6838\u5fc3\u903b\u8f91\uff09 health.py\uff08\u805a\u5408\u4e0e\u7edf\u8ba1\uff09 pipeline.py + document_processor.py\uff08\u6570\u636e\u8fdb\u5165\u7cfb\u7edf\u7684\u8def\u5f84\uff09 \u9700\u8981\u6211\u8fdb\u4e00\u6b65\u753b\u4e00\u5f20\u8c03\u7528\u94fe/\u65f6\u5e8f\u56fe\uff0c\u6216\u8005\u6311\u4e00\u4e2a\u5177\u4f53 API/\u5de5\u5177\u4ece\u8bf7\u6c42\u5230\u843d\u5e93\u7684\u5b8c\u6574\u8def\u5f84\u8bb2\u89e3\u5417\uff1f</p> <p>\u4e5f\u8bb8\u6211\u4eec\u6309\u7167\u6570\u636e\u6d41\u5411\u7684\u987a\u5e8f\u4e00\u4e2a\u6587\u4ef6\u4e00\u4e2a\u6587\u4ef6\u7684\u770b\uff1f\u6211\u60f3\u5bf9\u6574\u4e2a\u540e\u7aef\u770b\u7684\u6bd4\u8f83\u4ed4\u7ec6\u4e00\u70b9</p> <p>\u53ef\u4ee5\u3002\u6309\u6570\u636e\u6d41\u6211\u5efa\u8bae\u5206\u4e24\u6761\u8def\u7ebf\uff0c\u5148\u8d70\u6570\u636e\u8fdb\u5165\u7cfb\u7edf\u518d\u8d70\u67e5\u8be2/\u89c2\u6d4b\uff1a</p> <p>\u7d22\u5f15/\u5165\u5e93\u6d41\uff1apipeline.py  dataset_downloader.py  document_processor.py  utils.py  rag_engine.py  logging_utils.py  models.py \u67e5\u8be2/\u89c2\u6d4b\u6d41\uff1aserver.py  rag_engine.py  logging_utils.py  db.py  queries.py/health.py/search.py/websocket.py \u6211\u4eec\u5148\u4ece\u7d22\u5f15/\u5165\u5e93\u6d41\u7684\u5165\u53e3\u5f00\u59cb\u770b\u3002</p> <p>File 1: services/pipeline.py</p> <p>\u89d2\u8272\uff1a\u4e3b\u7f16\u6392\u5668\uff0c\u5b8c\u6210\u4e0b\u8f7d  \u89e3\u6790/\u5206\u5757  \u5199\u5165 ChromaDB\uff0c\u5e76\u6301\u7eed\u66f4\u65b0 indexing_jobs \u8868 \u5165\u53e3\uff1aPipeline.run()/run_async()\uff1bCLI main() \u652f\u6301\u53c2\u6570\uff08repo\u3001chunk \u5927\u5c0f\u3001\u5e76\u53d1\u6570\u3001chroma host \u7b49\uff09 Phase 1 \u4e0b\u8f7d\uff1a\u901a\u8fc7 GitHubDatasetDownloader \u83b7\u53d6\u4ed3\u5e93\u6587\u4ef6\uff0c\u7edf\u8ba1 total_files\uff0c\u65e0\u6587\u4ef6\u5219\u76f4\u63a5\u6807\u8bb0\u4efb\u52a1\u5931\u8d25 Phase 2 \u5904\u7406\uff1aDocumentProcessor \u5e76\u884c/\u4e32\u884c\u5904\u7406\uff0c\u6bcf\u4e2a\u6587\u4ef6\u4ea7\u51fa DocumentChunk\uff0c\u5b9e\u65f6\u66f4\u65b0\u8fdb\u5ea6\uff0c\u652f\u6301\u53d6\u6d88 Phase 3 \u5199\u5165\uff1a_index_chunks() \u4f7f\u7528 chromadb.HttpClient \u6216 HTTP API fallback \u6279\u91cf upsert\uff08\u53ea\u53d1 documents+metadatas\uff0c\u672a\u663e\u5f0f\u4f20 embeddings\uff0c\u4f9d\u8d56 Chroma \u4fa7\u7684 embedding \u884c\u4e3a\uff09 \u4efb\u52a1\u66f4\u65b0\uff1a_update_job() \u8d70\u539f\u751f SQL \u5148 UPDATE \u540e INSERT\u7684 upsert\uff0cDB \u4e0d\u53ef\u7528\u65f6\u4ec5 debug \u8bb0\u5f55\uff0c\u4e0d\u963b\u65ad\u6d41\u7a0b \u8865\u5145\uff1a\u8fd8\u6709\u4e00\u4e2a\u53ef\u9009\u5165\u53e3\u662f data_prep.py\uff08\u4ece DOJ ZIP \u4e0b\u8f7d+\u89e3\u538b+\u7d22\u5f15\uff09\uff0c\u5b83\u4f1a\u590d\u7528 Pipeline \u903b\u8f91\u3002</p> <p>\u5982\u679c\u8fd9\u4e2a\u987a\u5e8f OK\uff0c\u6211\u4e0b\u4e00\u6b65\u8bb2 dataset_downloader.py\u3002\u5982\u679c\u4f60\u60f3\u5148\u5207\u5230\u67e5\u8be2\u6d41\uff08server.py\uff09\u4e5f\u53ef\u4ee5\u8bf4\u3002</p> <p>\u67e5\u8be2\u89c2\u6d4b\u6d41</p> <p>\u67e5\u8be2/\u89c2\u6d4b\u6d41\uff08\u6587\u4ef6\u7ea7\u987a\u5e8f\uff09</p> <p>\u4e0b\u9762\u6309\u67e5\u8be2  \u8bb0\u5f55  \u89c2\u6d4b\u7684\u8def\u5f84\u6765\u8bb2\u3002\u6211\u5148\u4ece\u7b2c\u4e00\u4efd server.py \u8be6\u7ec6\u8bb2\u8d77\uff1b\u5982\u679c\u4f60\u89c9\u5f97\u8282\u594f\u5408\u9002\uff0c\u6211\u4e0b\u4e00\u6761\u7ee7\u7eed rag_engine.py\uff0c\u4e00\u76f4\u5f80\u540e\u8d70\u3002</p> <p>File 1: server.py\uff08\u67e5\u8be2\u5165\u53e3\u4e0e\u8c03\u5ea6\u4e2d\u5fc3\uff09</p> <p>\u89d2\u8272\uff1aMCP Server \u7684\u4e3b\u5165\u53e3\uff0c\u66b4\u9732\u5de5\u5177\uff08tools\uff09\u4e0e\u8d44\u6e90\uff08resources\uff09\uff0c\u628a\u67e5\u8be2/\u7d22\u5f15/\u72b6\u6001\u7b49\u8bf7\u6c42\u5206\u53d1\u5230\u5177\u4f53\u5b9e\u73b0\u3002 \u521d\u59cb\u5316\uff1a \u521b\u5efa Server \u5b9e\u4f8b\u4e0e RAGEngine\uff1aapp = Server(...)\uff0crag = RAGEngine() \u65e5\u5fd7\u7ea7\u522b\u7531 config.py \u63a7\u5236\u3002 \u5de5\u5177\u58f0\u660e\uff1aTOOLS \u5217\u8868\u5b9a\u4e49 8 \u4e2a\u5de5\u5177\uff08query_documents, search_similar, index_documents, check_status \u7b49\uff09\uff0cMCP \u5ba2\u6237\u7aef\u4f1a\u57fa\u4e8e\u8fd9\u4e2a schema \u8c03\u7528\u3002 \u8bf7\u6c42\u5165\u53e3\uff1a @app.call_tool()  _dispatch_tool() \u8fd9\u91cc\u662f\u67e5\u8be2\u6d41\u7684\u8d77\u70b9\uff1a_tool_query_documents() / _tool_search_similar() \u67e5\u8be2\u6d41\u7a0b\uff08\u6838\u5fc3\u8def\u5f84\uff09\uff1a _tool_query_documents(args) \u8bfb\u53d6 query/top_k \u8c03\u7528 rag.query()\uff08\u5728 rag_engine.py\uff09 \u7ec4\u88c5\u8fd4\u56de\u6587\u672c + sources \u8c03\u7528 log_query() \u5199\u5165 Postgres\uff08\u5728 logging_utils.py\uff09 \u8fd4\u56de JSON \u7ed3\u679c \u8d44\u6e90\uff08observability\uff09\uff1a @app.list_resources() / @app.read_resource() stats://queries, stats://jobs, stats://system \u7b49\u8d44\u6e90\u8bfb\u53d6\u7edf\u8ba1\u4fe1\u606f\uff0c\u5185\u90e8\u4e5f\u8c03\u7528 logging_utils \u4e0e rag \u76f8\u5173\u65b9\u6cd5\u3002 \u8fd0\u884c\u65b9\u5f0f\uff1a main() \u4f7f\u7528 MCP stdio \u8fd0\u884c\uff0c\u4e0d\u662f HTTP \u670d\u52a1\uff1b\u901a\u8fc7 python -m mcp_server \u542f\u52a8\u3002 \u5982\u679c\u4f60\u540c\u610f\u8fd9\u4e2a\u987a\u5e8f\uff0c\u6211\u4e0b\u4e00\u6761\u7ee7\u7eed\u770b rag_engine.py\uff08\u5411\u91cf\u68c0\u7d22\u7ec6\u8282 + ChromaDB \u63a5\u5165\uff09\uff0c\u7136\u540e\u662f logging_utils.py\uff08\u5199\u65e5\u5fd7/\u7edf\u8ba1\uff09\uff0c\u518d\u5230 dashboard_backend \u8bfb\u53d6\u8fd9\u4e9b\u65e5\u5fd7\u505a\u89c2\u6d4b\u3002</p> <p>\u4f60\u8fd8\u662f\u8bb2dashboard_backend\u90e8\u5206\u7684\u4ee3\u7801\u5427\uff0c\u6211\u60f3\u77e5\u9053\u8fd9\u91cc\u7684FASTapi\u662f\u505a\u4ec0\u4e48\u7684\uff0c\u4e3a\u4ec0\u4e48\u8981\u7528\u4ed6</p> <p>\u597d\u7684\uff0c\u6211\u4eec\u5c31\u805a\u7126 dashboard_backend\u3002\u6211\u4f1a\u5148\u8bf4\u660e\u5b83\u5728\u7cfb\u7edf\u91cc\u7684\u804c\u8d23\uff0c\u518d\u89e3\u91ca\u4e3a\u4ec0\u4e48\u7528 FastAPI\uff0c\u5e76\u7ed3\u5408\u5177\u4f53\u6587\u4ef6\u8bb2\u6e05\u695a\u5b83\u505a\u4ec0\u4e48\u3002</p> <p>dashboard_backend \u5728\u7cfb\u7edf\u91cc\u505a\u4ec0\u4e48</p> <p>\u5b83\u662f\u89c2\u6d4b\u5c42\u7684\u540e\u7aef\uff1a\u4ece Postgres \u8bfb\u53d6 MCP \u8bb0\u5f55\u7684\u67e5\u8be2\u65e5\u5fd7\u3001\u7d22\u5f15\u4efb\u52a1\u3001\u7cfb\u7edf\u6307\u6807\uff0c\u7136\u540e\u63d0\u4f9b\u7ed9\u524d\u7aef Dashboard\u3002 \u4e3b\u8981\u80fd\u529b\uff1a \u67e5\u8be2\u65e5\u5fd7 API\uff08\u5217\u8868\u3001\u8fc7\u6ee4\u3001\u7edf\u8ba1\uff09 \u7d22\u5f15\u4efb\u52a1\u76d1\u63a7\uff08\u8fdb\u5ea6\u3001\u8be6\u60c5\u3001\u53d6\u6d88\uff09 \u7cfb\u7edf\u5065\u5eb7\u4e0e\u6307\u6807\uff08DB\u3001\u5411\u91cf\u5e93\u3001\u6d3b\u8dc3\u8fde\u63a5\uff09 \u5206\u6790\u62a5\u8868\uff08\u8d8b\u52bf\u3001\u70ed\u529b\u56fe\u3001\u54cd\u5e94\u65f6\u95f4\u5206\u5e03\uff09 WebSocket \u5b9e\u65f6\u63a8\u9001 \u4e3a\u4ec0\u4e48\u7528 FastAPI\uff08\u800c\u4e0d\u662f Flask/Django\uff09</p> <p>\u5f02\u6b65\u53cb\u597d\uff1a\u8fd9\u91cc\u5927\u91cf\u662f DB \u67e5\u8be2 + HTTP \u8c03\u7528\uff08\u4f8b\u5982\u5065\u5eb7\u68c0\u67e5\u4e2d\u8bbf\u95ee ChromaDB\uff09\uff0cFastAPI \u7684 async \u652f\u6301\u8ba9 I/O \u66f4\u9ad8\u6548\u3002 \u5185\u7f6e\u6570\u636e\u6821\u9a8c/\u5e8f\u5217\u5316\uff1aPydantic \u81ea\u52a8\u6821\u9a8c\u8bf7\u6c42\u53c2\u6570\u5e76\u8f93\u51fa\u54cd\u5e94\u6a21\u578b\uff0c\u51cf\u5c11\u624b\u5199 schema\u3002 \u81ea\u52a8 API \u6587\u6863\uff1aFastAPI \u81ea\u5e26 /docs\uff0c\u5f88\u9002\u5408\u540e\u53f0\u7cfb\u7edf\u8c03\u8bd5\u3002 \u4f9d\u8d56\u6ce8\u5165\uff1aDepends(get_db) \u8ba9\u6570\u636e\u5e93\u4f1a\u8bdd\u7684\u751f\u547d\u5468\u671f\u7ba1\u7406\u66f4\u6e05\u6670\u3002 WebSocket \u652f\u6301\uff1a\u5b9e\u65f6 Dashboard \u7528\u5230 WebSocket\uff0cFastAPI \u539f\u751f\u652f\u6301\u3002 \u5177\u4f53\u4ee3\u7801\u600e\u4e48\u7ec4\u7ec7\uff08\u6309\u6587\u4ef6\uff09</p> <p>1) \u5165\u53e3\uff1amain.py    \u521b\u5efa FastAPI \u5e94\u7528\uff0c\u8bbe\u7f6e\u65e5\u5fd7\u3001CORS\u3001\u5f02\u5e38\u5904\u7406\u3001\u8def\u7531\u6ce8\u518c\u3002    \u751f\u547d\u5468\u671f lifespan()\uff1a    \u542f\u52a8\u65f6 init_db() \u521d\u59cb\u5316\u8868\uff08\u5f00\u53d1\u573a\u666f\uff09    \u5173\u95ed\u65f6 close_db() \u91ca\u653e\u8fde\u63a5\u6c60    \u6ce8\u518c\u8def\u7531\uff1a    queries, jobs, health, search, websocket    \u5b83\u7684\u5b9a\u4f4d\uff1a\u4e00\u4e2a\u603b\u7ebf\uff0c\u805a\u5408\u6240\u6709 API \u4e0e\u4e2d\u95f4\u4ef6\u3002 2) \u6570\u636e\u5e93\u4f1a\u8bdd\uff1adb.py    \u4f7f\u7528 sqlalchemy.ext.asyncio \u7684 create_async_engine\u3002    get_db() \u4f5c\u4e3a FastAPI \u4f9d\u8d56\uff0c\u81ea\u52a8\u7ed9\u6bcf\u4e2a\u8bf7\u6c42\u63d0\u4f9b AsyncSession\u3002    \u5b83\u7684\u5b9a\u4f4d\uff1a\u7edf\u4e00\u7ba1\u7406 DB \u8fde\u63a5\u4e0e\u4f1a\u8bdd\u590d\u7528\u3002 3) API \u8def\u7531    a) \u67e5\u8be2\u65e5\u5fd7\uff1aqueries.py</p> <p>/api/dashboard/queries\uff1a\u5206\u9875 + \u641c\u7d22 + \u65f6\u95f4\u8303\u56f4\u8fc7\u6ee4 /api/dashboard/queries/stats\uff1a\u7edf\u8ba1\uff08\u603b\u6570\u3001\u5e73\u5747/\u4e2d\u4f4d/P95\u3001\u8d8b\u52bf\u3001\u70ed\u95e8\u67e5\u8be2\u3001\u54cd\u5e94\u65f6\u95f4\u5206\u5e03\uff09 /api/dashboard/queries/{id}\uff1a\u67e5\u8be2\u8be6\u60c5 b) \u4efb\u52a1\u76d1\u63a7\uff1ajobs.py</p> <p>/api/dashboard/jobs\uff1a\u5217\u8868 + \u72b6\u6001\u8fc7\u6ee4 /api/dashboard/jobs/{id}\uff1a\u8be6\u60c5 /api/dashboard/jobs/{id}/progress\uff1a\u8fdb\u5ea6 + ETA /api/dashboard/jobs/{id}/cancel\uff1a\u53d6\u6d88\uff08\u5199 DB + websocket \u5e7f\u64ad\uff09 c) \u5065\u5eb7/\u6307\u6807/\u5206\u6790\uff1ahealth.py</p> <p>/api/dashboard/health\uff1a\u68c0\u67e5 DB + MCP + ChromaDB \u72b6\u6001 /api/dashboard/metrics\uff1a\u8bfb\u53d6 system_metrics\uff0c\u8f93\u51fa CPU/MEM/Disk + \u6d3b\u8dc3\u8fde\u63a5 /api/dashboard/analytics\uff1a\u8d8b\u52bf\u3001\u70ed\u529b\u56fe\u3001\u6587\u6863\u7c7b\u578b\u5206\u5e03\u7b49 d) \u641c\u7d22\uff1asearch.py</p> <p>/api/dashboard/search\uff1a\u76f4\u63a5\u8c03\u7528 ChromaDB \u641c\u7d22\u5e76\u5199\u5165 query_logs \u8fd9\u662f\u4e2a\u7528\u6237\u5728 Dashboard \u4e0a\u8fdb\u884c\u641c\u7d22\u7684\u5165\u53e3 e) WebSocket\uff1awebsocket.py</p> <p>/ws/dashboard\uff1a\u5b9e\u65f6\u63a8\u9001\u67e5\u8be2/\u4efb\u52a1/\u6307\u6807\u66f4\u65b0 \u5185\u7f6e heartbeat \u4fdd\u6d3b\u673a\u5236 4) Pydantic \u6a21\u578b\uff1aschemas.py \u6240\u6709 API \u7684\u54cd\u5e94\u6a21\u578b\u5728\u8fd9\u91cc\u5b9a\u4e49 \u597d\u5904\uff1a\u8fd4\u56de\u7ed3\u6784\u56fa\u5b9a\uff0c\u524d\u7aef\u6709\u5f3a\u7ea6\u675f\u5951\u7ea6 5) \u914d\u7f6e\uff1aconfig.py \u4f7f\u7528 pydantic_settings.BaseSettings \u73af\u5883\u53d8\u91cf\u7edf\u4e00 DASHBOARD_ \u524d\u7f00\uff08\u4e0e MCP \u914d\u7f6e\u89e3\u8026\uff09 \u4e00\u53e5\u8bdd\u603b\u7ed3 FastAPI \u5728\u8fd9\u91cc\u7684\u6838\u5fc3\u4ef7\u503c\u662f\uff1a\u5f02\u6b65 + \u5f3a\u7c7b\u578b + WebSocket + \u81ea\u52a8\u6587\u6863\uff0c\u9002\u5408\u6784\u5efa \u5b9e\u65f6\u89c2\u6d4b\u540e\u53f0 API\u3002</p> <p>\u5982\u679c\u4f60\u60f3\u9010\u6587\u4ef6\u6df1\u8bfb\uff0c\u6211\u5efa\u8bae\u4ece main.py  db.py  health.py  queries.py \u7684\u987a\u5e8f\u5f00\u59cb\u3002 \u4f60\u60f3\u7ee7\u7eed\u4ece\u54ea\u4e00\u4e2a\u6587\u4ef6\u6df1\u5165\uff1f</p> <p>main.py\u5f00\u59cb\uff0c\u6211\u662f\u4e00\u4e2a\u6ca1\u63a5\u89e6\u8fc7\u540e\u7aef\u53ea\u77e5\u8bc6\u7684\u65b0\u624b</p> <p>\u597d\u7684\uff0c\u6211\u4eec\u5c31\u4ece main.py \u5f00\u59cb\uff0c\u7528\u65b0\u624b\u80fd\u61c2\u7684\u65b9\u5f0f\u8bb2\u6e05\u695a\u5b83\u5728\u505a\u4ec0\u4e48\u3001\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5199\u3002</p> <p>\u8fd9\u4efd\u6587\u4ef6\u7684\u4f5c\u7528\uff1a\u6574\u4e2a\u540e\u7aef API \u7684\u603b\u5165\u53e3 \u5b83\u5c31\u50cf\u4e00\u4e2a\u524d\u53f0\u603b\u53f0\uff1a</p> <p>\u542f\u52a8\u65f6\u505a\u51c6\u5907\u5de5\u4f5c \u6ce8\u518c\u4e00\u5806\u63a5\u53e3 \u63a5\u53d7\u8bf7\u6c42\u3001\u8bb0\u5f55\u65e5\u5fd7\u3001\u5904\u7406\u9519\u8bef \u5141\u8bb8\u524d\u7aef\u8bbf\u95ee\uff08CORS\uff09 \u4e0b\u9762\u6309\u4ee3\u7801\u987a\u5e8f\u89e3\u91ca\uff1a</p> <p>1) \u5bfc\u5165\u4f9d\u8d56    from fastapi import FastAPI, Request    from fastapi.middleware.cors import CORSMiddleware    from fastapi.responses import JSONResponse    FastAPI\uff1a\u521b\u5efa API \u670d\u52a1\u672c\u4f53    Request\uff1a\u7528\u4e8e\u8bfb\u53d6\u8bf7\u6c42\u4fe1\u606f\uff08\u6bd4\u5982\u65e5\u5fd7\u91cc\u8981\u6253\u5370\u8def\u5f84\uff09    CORSMiddleware\uff1a\u8ba9\u524d\u7aef\u7f51\u9875\u80fd\u8de8\u57df\u8bbf\u95ee\u8fd9\u4e2a\u540e\u7aef    JSONResponse\uff1a\u51fa\u9519\u65f6\u8fd4\u56de JSON \u6570\u636e 2) \u914d\u7f6e\u4e0e\u751f\u547d\u5468\u671f\uff08\u542f\u52a8\u548c\u5173\u95ed\uff09    @asynccontextmanager    async def lifespan(app: FastAPI):    logging.basicConfig(...)    await init_db()    yield    await close_db()    \u542f\u52a8\u65f6\uff1a    \u914d\u65e5\u5fd7    init_db() \u521d\u59cb\u5316\u6570\u636e\u5e93\u8868\uff08\u53ea\u5728\u5f00\u53d1\u9636\u6bb5\u6709\u7528\uff09    \u5173\u95ed\u65f6\uff1a    close_db() \u91ca\u653e\u8fde\u63a5\u6c60    \u4f60\u53ef\u4ee5\u7406\u89e3\u4e3a\uff1a\u670d\u52a1\u5f00\u673a\u5148\u51c6\u5907\u6570\u636e\u5e93\u8fde\u63a5\uff0c\u5173\u673a\u65f6\u6536\u5c3e\u3002 3) \u521b\u5efa FastAPI \u5e94\u7528    app = FastAPI(    title=\"Epstein RAG Dashboard API\",    ...    lifespan=lifespan,    )    \u8fd9\u5c31\u662f \u6574\u4e2a\u540e\u7aef\u670d\u52a1\u5b9e\u4f8b    lifespan \u8ba9\u670d\u52a1\u542f\u52a8/\u9000\u51fa\u65f6\u81ea\u52a8\u6267\u884c\u524d\u9762\u7684\u51c6\u5907/\u6536\u5c3e\u903b\u8f91 4) CORS \u8de8\u57df\u914d\u7f6e    app.add_middleware(    CORSMiddleware,    allow_origins=settings.cors_origins,    ...    )    \u4e3a\u4ec0\u4e48\u9700\u8981\uff1f    \u524d\u7aef\u662f localhost (line 3000)\uff0c\u540e\u7aef\u662f localhost (line 8001)\uff0c\u6d4f\u89c8\u5668\u9ed8\u8ba4\u4e0d\u5141\u8bb8\u8de8\u57df\u8bf7\u6c42\u3002    CORS \u5c31\u662f\u544a\u8bc9\u6d4f\u89c8\u5668\uff1a\u8fd9\u4e2a\u540e\u7aef\u5141\u8bb8\u8fd9\u4e9b\u524d\u7aef\u57df\u540d\u8bbf\u95ee\u3002 5) \u8bf7\u6c42\u65e5\u5fd7\u4e2d\u95f4\u4ef6    @app.middleware(\"http\")    async def log_requests(request: Request, call_next):    logger.debug(\"%s %s\", request.method, request.url.path)    response = await call_next(request)    return response    \u6bcf\u6b21\u8bf7\u6c42\u8fdb\u6765\uff0c\u90fd\u6253\u5370\u4e00\u4e0b\u8bf7\u6c42\u65b9\u5f0f\u548c\u8def\u5f84    \u65b9\u4fbf\u8c03\u8bd5\uff0c\u6bd4\u5982\u4f60\u80fd\u770b\u5230 GET /api/dashboard/queries 6) \u5168\u5c40\u5f02\u5e38\u5904\u7406    @app.exception_handler(Exception)    async def global_exception_handler(request, exc):    return JSONResponse(status_code=500, content={\"detail\": \"Internal server error\"})    \u907f\u514d\u670d\u52a1\u5668\u5d29\u6389    \u7edf\u4e00\u8fd4\u56de JSON \u9519\u8bef\u4fe1\u606f\uff0c\u524d\u7aef\u66f4\u597d\u5904\u7406 7) \u6ce8\u518c API \u8def\u7531    app.include_router(queries.router)    app.include_router(jobs.router)    app.include_router(health.router)    app.include_router(search.router)    app.include_router(websocket.router)    \u8fd9\u4e00\u6b65\u76f8\u5f53\u4e8e\u628a\u5404\u4e2a\u6a21\u5757\u7684\u529f\u80fd\u6302\u5230\u603b\u5165\u53e3\u4e0a\u3002    \u6bcf\u4e2a router \u662f\u4e00\u4e2a\u529f\u80fd\u7ec4\uff0c\u6bd4\u5982\uff1a</p> <p>\u67e5\u8be2\u65e5\u5fd7 \u7d22\u5f15\u4efb\u52a1 \u5065\u5eb7\u68c0\u67e5 \u641c\u7d22 WebSocket \u5b9e\u65f6\u6d88\u606f 8) \u6839\u8def\u5f84 / @app.get(\"/\") async def root():     return {...} \u8bbf\u95ee http://localhost:8001/ \u53ef\u4ee5\u770b\u5230\u670d\u52a1\u8bf4\u660e\u548c\u6709\u54ea\u4e9b\u63a5\u53e3\u3002</p> <p>\u4e00\u53e5\u8bdd\u603b\u7ed3 main.py \u5c31\u662f \u540e\u7aef\u670d\u52a1\u7684\u542f\u52a8\u5668 + \u603b\u5165\u53e3\uff1a</p> <p>\u542f\u52a8\u65f6\u51c6\u5907\u6570\u636e\u5e93 \u6ce8\u518c API \u5904\u7406\u8de8\u57df\u3001\u65e5\u5fd7\u3001\u5f02\u5e38 \u628a\u8def\u7531\u6302\u5230\u540c\u4e00\u4e2a\u670d\u52a1\u5668\u4e0a \u8981\u7ee7\u7eed\u7684\u8bdd\uff0c\u6211\u5efa\u8bae\u4e0b\u4e00\u6b65\u770b queries.py\uff08\u67e5\u8be2\u65e5\u5fd7\u63a5\u53e3\uff09\u6216 health.py\uff08\u5065\u5eb7\u4e0e\u6307\u6807\uff09\u3002 \u4f60\u60f3\u4ece\u54ea\u4e00\u4e2a\u7ee7\u7eed\uff1f</p> <p>queries.py</p> <p>\u597d\u7684\uff0c\u6211\u4eec\u770b queries.py\u3002\u8fd9\u4efd\u6587\u4ef6\u8d1f\u8d23 \u67e5\u8be2\u65e5\u5fd7\u7684 API\uff0c\u4e5f\u5c31\u662f\u524d\u7aef\u60f3\u770b\u7528\u6237\u95ee\u4e86\u4ec0\u4e48\u3001\u54cd\u5e94\u591a\u5feb\u3001\u70ed\u95e8\u95ee\u9898\u6709\u54ea\u4e9b\u7b49\u6570\u636e\u65f6\u4f1a\u8bbf\u95ee\u8fd9\u91cc\u3002</p> <p>\u6211\u4f1a\u6309\u65b0\u624b\u80fd\u61c2\u7684\u65b9\u5f0f\u4ece\u4e0a\u5230\u4e0b\u8bb2\u3002</p> <p>File: queries.py</p> <p>1) \u8fd9\u4e2a\u6587\u4ef6\u7684\u76ee\u7684    \u5b83\u63d0\u4f9b\u4e09\u7c7b\u63a5\u53e3\uff1a</p> <p>\u67e5\u8be2\u5217\u8868\uff1a\u6700\u8fd1\u67e5\u8be2\u65e5\u5fd7\uff08\u5e26\u5206\u9875\u3001\u8fc7\u6ee4\uff09 \u67e5\u8be2\u7edf\u8ba1\uff1a\u603b\u91cf\u3001\u5e73\u5747/\u4e2d\u4f4d/P95\u3001\u8d8b\u52bf\u3001\u70ed\u95e8\u67e5\u8be2 \u67e5\u8be2\u8be6\u60c5\uff1a\u5355\u6761\u67e5\u8be2\u8bb0\u5f55 2) \u5173\u952e\u4f9d\u8d56 from fastapi import APIRouter, Depends, HTTPException, Query from sqlalchemy import desc, func, text, case, literal from sqlalchemy.ext.asyncio import AsyncSession from sqlalchemy.future import select APIRouter\uff1a\u628a\u8fd9\u4e00\u7ec4\u63a5\u53e3\u5f52\u7c7b Depends(get_db)\uff1a\u81ea\u52a8\u62ff\u5230\u6570\u636e\u5e93\u4f1a\u8bdd select + func\uff1aSQLAlchemy \u7684\u67e5\u8be2\u6784\u5efa\u5668 Query(...)\uff1a\u58f0\u660e URL \u53c2\u6570\uff0c\u5e26\u6821\u9a8c/\u9ed8\u8ba4\u503c 3) \u8def\u7531\u524d\u7f00 router = APIRouter(prefix=\"/api/dashboard/queries\", tags=[\"queries\"]) \u6240\u4ee5\u6240\u6709\u8def\u5f84\u90fd\u4ee5 /api/dashboard/queries \u5f00\u5934\u3002</p> <p>\u6838\u5fc3\u63a5\u53e3\u8bb2\u89e3 A. \u67e5\u8be2\u5217\u8868\uff1aGET /api/dashboard/queries async def get_recent_queries(     limit=50, offset=0, search=None, client_type=None, time_range=None, db=Depends(get_db) ) \u7528\u9014\uff1a\u83b7\u53d6\u67e5\u8be2\u65e5\u5fd7\u5217\u8868</p> <p>\u53c2\u6570\u542b\u4e49</p> <p>limit\uff1a\u6bcf\u9875\u591a\u5c11\u6761\uff08\u9ed8\u8ba4 50\uff09 offset\uff1a\u4ece\u7b2c\u51e0\u6761\u5f00\u59cb search\uff1a\u641c\u7d22\u5173\u952e\u8bcd\uff08\u5bf9 query_text \u505a\u6a21\u7cca\u5339\u914d\uff09 client_type\uff1a\u8fc7\u6ee4\u6765\u6e90\uff08\u6bd4\u5982 dashboard \u6216 mcp\uff09 time_range\uff1a\u65f6\u95f4\u8303\u56f4\uff08\u5982 24h, 7d\uff09 \u67e5\u8be2\u6b65\u9aa4</p> <p>query = select(QueryLog) \u6839\u636e search\u3001client_type\u3001time_range \u52a0\u8fc7\u6ee4\u6761\u4ef6 \u5148\u7528 count() \u62ff\u603b\u91cf \u518d\u6392\u5e8f + \u5206\u9875 \u8fd4\u56de QueryListResponse \u8fd4\u56de\u7ed3\u6784</p> <p>{   \"queries\": [...],   \"total\": 1234,   \"page\": 1,   \"page_size\": 50 } B. \u67e5\u8be2\u7edf\u8ba1\uff1aGET /api/dashboard/queries/stats async def get_query_statistics(time_range=\"24h\", db=Depends(get_db)) \u7528\u9014\uff1a\u7edf\u8ba1\u5206\u6790\u6570\u636e</p> <p>\u8fd4\u56de\u5185\u5bb9\u5305\u62ec\uff1a</p> <p>total_queries\uff1a\u603b\u6b21\u6570 avg_response_time_ms\uff1a\u5e73\u5747\u54cd\u5e94\u65f6\u95f4 median_response_time_ms\uff1a\u4e2d\u4f4d\u6570 p95_response_time_ms\uff1aP95 query_trend\uff1a\u6309\u5c0f\u65f6\u5206\u6876\u7684\u8d8b\u52bf popular_queries\uff1a\u6700\u5e38\u89c1\u67e5\u8be2 response_time_distribution\uff1a\u54cd\u5e94\u65f6\u95f4\u5206\u5e03 \u91cd\u70b9\u903b\u8f91</p> <p>func.percentile_cont \u8ba1\u7b97\u4e2d\u4f4d\u6570\u548c P95\uff08\u8fd9\u662f PostgreSQL \u7684\u7edf\u8ba1\u51fd\u6570\uff09 \u7528 date_trunc(\"hour\", ...) \u6309\u5c0f\u65f6\u7edf\u8ba1 \u7528 case() \u7edf\u8ba1\u54cd\u5e94\u65f6\u95f4\u5206\u6876\uff08&lt;0.5s, 0.5-1s, \uff09 C. \u67e5\u8be2\u8be6\u60c5\uff1aGET /api/dashboard/queries/{query_id} async def get_query_detail(query_id: UUID, db=Depends(get_db)) \u7528\u9014\uff1a\u83b7\u53d6\u5355\u6761\u67e5\u8be2\u65e5\u5fd7</p> <p>\u5982\u679c\u627e\u4e0d\u5230\uff0c\u4f1a raise HTTPException(404) 4) \u65f6\u95f4\u8303\u56f4\u89e3\u6790 def _parse_time_range(time_range: str) -&gt; str | None:     mapping = {         \"1h\": \"1 hour\",         \"24h\": \"24 hours\",         \"7d\": \"7 days\",         ...     } \u8fd9\u53ea\u662f\u628a\u524d\u7aef\u4f20\u6765\u7684\u77ed\u683c\u5f0f\u53d8\u6210 SQL \u80fd\u7406\u89e3\u7684 interval\u3002</p> <p>\u65b0\u624b\u53ef\u4ee5\u8fd9\u6837\u7406\u89e3 queries.py \u5c31\u662f\u67e5\u65e5\u5fd7\u7684 API \u5b83\u4e0d\u4f1a\u81ea\u5df1\u53bb\u8bb0\u5f55\u65e5\u5fd7\uff0c\u800c\u662f\u8bfb\u53d6 query_logs \u8868\u91cc\u7684\u5386\u53f2\u8bb0\u5f55 \u7edf\u8ba1\u63a5\u53e3\u53ea\u662f\u5bf9\u8fd9\u4e9b\u8bb0\u5f55\u505a\u5206\u7ec4\u548c\u805a\u5408 \u5982\u679c\u4f60\u60f3\u7ee7\u7eed\uff0c\u6211\u63a8\u8350\u4e0b\u4e00\u4e2a\u6587\u4ef6\u662f\uff1a</p> <p>health.py\uff08\u7cfb\u7edf\u5065\u5eb7\u4e0e\u4eea\u8868\u76d8\u6307\u6807\uff09 search.py\uff08Dashboard \u91cc\u76f4\u63a5\u641c\u7d22\u7684\u5165\u53e3\uff09 \u4f60\u60f3\u5148\u770b\u54ea\u4e2a\uff1f</p> <p>\u6240\u4ee5\u8fd9\u91cc\u7684fastapi\u5c31\u50cf\u662f\uff0c\u5b9a\u4e49\u4e00\u4e0b\u7f51\u9875\u91cc\u4e0d\u540c\u7684\u5b50\u9875\u9762\u90fd\u662f\u505a\u4ec0\u4e48\u7684\uff0c\u7136\u540e\u6bcf\u4e2a\u5b50\u9875\u9762\u90a3\u4e9b\u6309\u94ae\u70b9\u4e0b\u53bb\u662f\u8c03\u7528\u4e86\u4ec0\u4e48\u51fd\u6570\uff1f</p> <p>\u5dee\u4e0d\u591a\u662f\u8fd9\u4e2a\u610f\u601d\uff0c\u4f46\u9700\u8981\u7a0d\u5fae\u7ea0\u6b63\u4e00\u4e0b\uff1a</p> <p>FastAPI \u5728\u8fd9\u91cc\u4e0d\u662f\u5b9a\u4e49\u7f51\u9875\u9875\u9762\uff0c\u800c\u662f\u5b9a\u4e49\u7f51\u9875\u4f1a\u53bb\u8bf7\u6c42\u7684\u63a5\u53e3\u3002</p> <p>\u524d\u7aef\u9875\u9762\uff08React \u91cc\u7684 SystemHealth.tsx, StatCard.tsx, ResponseTimeChart.tsx\uff09\u8d1f\u8d23\u753b\u754c\u9762\u548c\u6309\u94ae\uff1b FastAPI \u8d1f\u8d23\u63d0\u4f9b\u6570\u636e\u548c\u52a8\u4f5c\u7684\u5165\u53e3\u3002</p> <p>\u53ef\u4ee5\u8fd9\u6837\u5bf9\u6bd4\u7406\u89e3\uff1a</p> <p>\u4f60\u770b\u5230\u7684\u4e1c\u897f \u5c5e\u4e8e\u8c01 \u4f5c\u7528 \u9875\u9762\u5e03\u5c40/\u5361\u7247/\u56fe\u8868 \u524d\u7aef\uff08React\uff09 \u753b\u51fa\u6765\u3001\u5c55\u793a\u7ed9\u7528\u6237 \u70b9\u51fb\u6309\u94ae\u540e\u83b7\u53d6\u6570\u636e \u524d\u7aef\u8c03\u7528 \u53d1\u9001\u8bf7\u6c42 /api/dashboard/queries \u540e\u7aef\uff08FastAPI\uff09 \u8fd4\u56de\u67e5\u8be2\u65e5\u5fd7 /api/dashboard/health \u540e\u7aef\uff08FastAPI\uff09 \u8fd4\u56de\u7cfb\u7edf\u5065\u5eb7\u72b6\u6001 \u66f4\u8d34\u8fd1\u4f60\u7684\u8bf4\u6cd5\uff1a FastAPI \u7684\u8def\u7531 = \u524d\u7aef\u529f\u80fd\u7684\u5165\u53e3 \u524d\u7aef\u6309\u94ae/\u9875\u9762 = \u89e6\u53d1\u8fd9\u4e9b\u5165\u53e3\u7684\u5730\u65b9 \u6bd4\u5982\uff1a</p> <p>\u4f8b\u5b50 1\uff1a\u9875\u9762\u52a0\u8f7d\u5065\u5eb7\u72b6\u6001 \u524d\u7aef SystemHealth.tsx \u4f1a\u8bf7\u6c42\uff1a</p> <p>GET /api/dashboard/health FastAPI \u91cc\u5bf9\u5e94\uff1a</p> <p>@router.get(\"/health\") async def get_system_health(...) \u4f8b\u5b50 2\uff1a\u67e5\u8be2\u65e5\u5fd7\u9875\u9762 \u524d\u7aef\u53ef\u80fd\u8bf7\u6c42\uff1a</p> <p>GET /api/dashboard/queries?limit=50 FastAPI \u91cc\u5bf9\u5e94\uff1a</p> <p>@router.get(\"\") async def get_recent_queries(...) \u6240\u4ee5\u4e00\u53e5\u8bdd\u603b\u7ed3\uff1a FastAPI \u4e0d\u8d1f\u8d23\u9875\u9762\u662f\u4ec0\u4e48\u6837\u5b50\uff0c\u800c\u662f\u8d1f\u8d23\u9875\u9762\u9700\u8981\u7684\u6570\u636e\u4ece\u54ea\u6765\u3001\u600e\u4e48\u62ff\u3002</p> <p>\u5982\u679c\u4f60\u613f\u610f\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u67d0\u4e2a\u524d\u7aef\u7ec4\u4ef6\u548c\u540e\u7aef\u63a5\u53e3\u5bf9\u4e0a\uff0c\u6bd4\u5982\u4ece SystemHealth.tsx \u770b\u5b83\u8bf7\u6c42\u4e86\u4ec0\u4e48 API\uff0c\u7136\u540e\u8df3\u56de health.py \u4e00\u4e00\u5bf9\u5e94\u8bb2\u3002</p> <p>\u90a3\u6211\u70b9\u4e86\u4e00\u4e2a\u6309\u94ae\uff0c\u7f51\u9875\u7684url\u6539\u53d8\u4e86\uff0c\u8fd9\u662f\u4ec0\u4e48\u73b0\u8c61</p> <p>\u8fd9\u662f\u524d\u7aef\u8def\u7531\uff08clientside routing\uff09\u7684\u73b0\u8c61\u3002</p> <p>\u7b80\u5355\u7406\u89e3\uff1a</p> <p>URL \u53d8\u4e86\uff0c\u4f46\u9875\u9762\u6ca1\u6574\u9875\u5237\u65b0  \u591a\u534a\u662f\u524d\u7aef\uff08React\uff09\u81ea\u5df1\u5728\u5207\u6362\u9875\u9762\uff0c\u4e0d\u662f\u540e\u7aef\u5728\u8fd4\u56de\u65b0\u9875\u9762\u3002 URL \u53d8\u4e86\uff0c\u800c\u4e14\u6574\u9875\u91cd\u65b0\u52a0\u8f7d  \u8fd9\u624d\u662f\u4f20\u7edf\u540e\u7aef\u8def\u7531\u3002 \u5728 React \u91cc\u901a\u5e38\u662f\u8fd9\u6837\u89e6\u53d1\u7684\uff1a</p> <p>\u70b9\u51fb\u6309\u94ae  navigate(\"/queries\") \u6216\u8005 <code>&lt;Link to=\"/queries\"&gt;</code> \u6d4f\u89c8\u5668\u5730\u5740\u680f\u53d8\u4e86\uff0c\u4f46\u5b9e\u9645\u4e0a\u53ea\u662f\u524d\u7aef\u7ec4\u4ef6\u5207\u6362 \u5982\u679c URL \u53d8\u5316\u5e26 ? \u53c2\u6570\uff08\u6bd4\u5982 ?page=2\uff09\uff0c\u4e5f\u53ef\u80fd\u53ea\u662f\u72b6\u6001\u53d8\u5316\uff0c\u5e76\u4e0d\u4e00\u5b9a\u6362\u9875\u9762\u3002</p> <p>\u8981\u4e0d\u8981\u6211\u5e2e\u4f60\u5728\u524d\u7aef\u91cc\u5b9a\u4f4d\u662f\u54ea\u4e00\u884c\u4ee3\u7801\u505a\u4e86\u8def\u7531\u5207\u6362\uff1f\u901a\u5e38\u5728 main.tsx \u6216 App.tsx \u91cc\u3002</p> <p>SystemHealth.tsx \u770b\u5b83\u8bf7\u6c42\u4e86\u4ec0\u4e48 API\uff0c\u7136\u540e\u8df3\u56de health.py \u4e00\u4e00\u5bf9\u5e94\u8bb2\u3002</p> <p>\u5df2\u5904\u7406 54s \u4e0b\u9762\u6211\u6309\u524d\u7aef  API  \u540e\u7aef\u51fd\u6570\u4e00\u4e00\u5bf9\u5e94\u8bb2\u6e05\u695a\uff1a</p> <p>\u8bf7\u6c42\u94fe\u8def\uff08SystemHealth.tsx  FastAPI\uff09</p> <p>SystemHealth.tsx useEffect \u91cc\u8c03\u7528 fetchHealth()\uff0c\u5e76\u4e14\u6bcf 30 \u79d2\u8f6e\u8be2\u4e00\u6b21 api.ts fetchHealth()  api.get(\"/health\") api \u7684 baseURL \u662f \"/api/dashboard\" \u6240\u4ee5\u5b9e\u9645\u8bf7\u6c42\u8def\u5f84\u662f\uff1aGET /api/dashboard/health \u540e\u7aef\u5bf9\u5e94\uff1ahealth.py @router.get(\"/health\")  get_system_health(...) SystemHealth.tsx \u771f\u6b63\u4f9d\u8d56\u7684\u8fd4\u56de\u5b57\u6bb5 \u5b83\u4e3b\u8981\u7528\u7684\u662f\u8fd9\u4e24\u4e2a\uff1a</p> <p>health.components\uff08\u9010\u6761\u663e\u793a\u7cfb\u7edf\u7ec4\u4ef6\u72b6\u6001\uff09 \u53ef\u9009\u7684 health.metrics.disk_usage\uff08&gt;80% \u65f6\u663e\u793a\u78c1\u76d8\u8b66\u544a\uff09 health.py \u91cc /health \u505a\u4e86\u4ec0\u4e48 health.py \u7684 get_system_health \u51fd\u6570\u4f1a\uff1a</p> <p>\u68c0\u67e5 PostgreSQL\uff1aSELECT 1 \u68c0\u67e5 MCP Server\uff1a\u770b query_logs \u662f\u5426\u6709\u6700\u65b0\u8bb0\u5f55 \u68c0\u67e5 Indexing Jobs\uff1a\u7edf\u8ba1 processing \u7684\u4efb\u52a1\u6570 \u68c0\u67e5 ChromaDB\uff1a\u8bf7\u6c42 collection \u548c count \u6c47\u603b\u603b\u4f53\u72b6\u6001\uff1a \u6709 error  unhealthy \u6709 warning/unknown  degraded \u5426\u5219 healthy \u8fd4\u56de\u7ed3\u6784\uff08\u7b80\u5316\uff09\uff1a {   \"status\": \"healthy|degraded|unhealthy\",   \"uptime_seconds\": 123.4,   \"components\": [     {\"name\":\"PostgreSQL\",\"status\":\"connected\"},     {\"name\":\"MCP Server\",\"status\":\"running\"},     {\"name\":\"Indexing Engine\",\"status\":\"idle\"},     {\"name\":\"Vector Database\",\"status\":\"connected\"}   ] } \u6ce8\u610f\u4e00\u4e2a\u5c0f\u7ec6\u8282</p> <p>SystemHealth.tsx \u91cc\u4f1a\u68c0\u67e5 health.metrics.disk_usage\uff0c\u4f46 /health \u5e76\u4e0d\u8fd4\u56de metrics\u3002 \u771f\u6b63\u8fd4\u56de metrics \u7684\u662f GET /api/dashboard/metrics\uff08\u4e5f\u5728 health.py \u91cc\uff09\u3002 \u6240\u4ee5\u73b0\u5728\u78c1\u76d8\u62a5\u8b66\u57fa\u672c\u4e0d\u4f1a\u89e6\u53d1\uff0c\u9664\u975e\u4f60\u628a /metrics \u7684\u7ed3\u679c\u5408\u5e76\u8fdb\u6765\uff0c\u6216\u8005\u6269\u5c55 /health \u8fd4\u56de metrics\u3002</p>","tags":["backend","fastapi","dashboard","rag"]},{"location":"2026/2026-02/2026-02-25-git-apply-failure-behavior/","title":"Summary note (git apply failure behavior)","text":"","tags":["git","patch","diff","apply"]},{"location":"2026/2026-02/2026-02-25-git-apply-failure-behavior/#summary","title":"Summary","text":"<ol> <li>git apply can relocate a hunk when original line/context no longer matches but another matching context exists later; line numbers are not strict anchors.</li> <li>In this case, a hunk intended for early cram_io.c matched a later repeated context and applied with a large offset.</li> <li>Malformed unified diff content (for example a raw empty/body line without diff prefix) can be effectively treated as trailing garbage; part of the hunk may still apply, which makes location/debugging harder.</li> <li>Your recent fix direction is correct: enforce strict reverse apply checks and reject offset/fuzz outcomes, and sanitize generated hunk body lines before apply.</li> </ol>","tags":["git","patch","diff","apply"]},{"location":"2026/2026-02/2026-02-25-git-apply-failure-behavior/#reproducible-code","title":"Reproducible code","text":"<pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nworkdir=\"$(mktemp -d)\"\necho \"WORKDIR=$workdir\"\ncd \"$workdir\"\ngit init -q\n\necho \"== Case 1: Hunk relocates to wrong repeated context ==\"\ncat &gt; demo.txt &lt;&lt;'TXT'\nSTART\nalpha\nbeta\ngamma\ndelta\nepsilon\nzeta\nEND1\n\nalpha\nbeta\ngamma\ndelta\nepsilon\nzeta\nEND2\nTXT\n\ngit add demo.txt\ngit commit -q -m init\n\n# Build patch targeting first block\nperl -0777 -i -pe 's/gamma\\n/gamma\\nINJECTED\\n/' demo.txt\ngit diff &gt; insert.patch\n\n# Break first block only; second block still matches\ngit checkout -- demo.txt\nperl -0777 -i -pe 's/delta\\n/delta_changed\\n/' demo.txt\n\ngit apply --check insert.patch\ngit apply --verbose insert.patch\nnl -ba demo.txt | sed -n '1,30p'\n\necho\necho \"== Case 2: malformed trailing diff text still passes git apply check ==\"\nprintf 'one\\ntwo\\nthree\\n' &gt; a.txt\ngit add a.txt\ngit commit -q -m add_a\n\ncat &gt; broken.patch &lt;&lt;'PATCH'\ndiff --git a/a.txt b/a.txt\n--- a/a.txt\n+++ b/a.txt\n@@ -1,3 +1,4 @@\n one\n+X\n two\n three\nBROKEN_TRAILING_LINE\nPATCH\n\ngit apply --check broken.patch\npatch -p1 --dry-run --verbose &lt; broken.patch || true\ngit apply broken.patch\nnl -ba a.txt\n</code></pre>","tags":["git","patch","diff","apply"]},{"location":"2026/2026-02/2026-02-25-git-apply-failure-behavior/#expected-key-output","title":"Expected key output","text":"<ul> <li>Case 1: Hunk #1 succeeded at ... (offset ... lines) and INJECTED appears in second block.</li> <li>Case 2: patch reports Ignoring the trailing garbage, while git apply --check still passes.</li> </ul>","tags":["git","patch","diff","apply"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/","title":"2026 02 27 multiplivault echidna fuzzing rounding a c86fa4ad","text":"","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#multiplivault-echidna-fuzzing-rounding-analysis","title":"MultipliVault Echidna Fuzzing &amp; Rounding Analysis","text":"","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#context","title":"Context","text":"<p>Bug bounty on HackenProof targeting Multipli smart contracts on Avalanche C-Chain. Rewards: Critical \\(5k-\\)10k, High \\(2k-\\)5k, Medium \\(750-\\)2k, Low \\(0-\\)750. PoC required (Foundry preferred).</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#in-scope-contracts","title":"In-Scope Contracts","text":"Contract Address MultipliVault (impl) <code>0xb63601A11c5bDC79D511B8F73871d7C0d8B57AE9</code> ERC1967Proxy (xUSDC) <code>0xCF0Eb4ac018C06a16ED5c63484823C7805e7599D</code> VariableVaultFee <code>0x4E5FEa916ef8458b8D877BD760B6930Fb4f28B72</code> VaultFundManager <code>0x01e676EAA0C9780A88395c651349Cf08Fe52368e</code> RolesAuthority <code>0xf580B985e2Fd8A8b0e4a56C2a7E24bC28e872609</code>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#echidna-setup-journey","title":"Echidna Setup Journey","text":"<p>Getting Echidna to deploy MultipliVault (a UUPS upgradeable ERC4626 vault with async redeem) required solving several non-obvious issues:</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#problem-1-invalid-b16-decoding","title":"Problem 1: \"invalid b16 decoding\"","text":"<p>Echidna's <code>deployBytecodes</code> expects raw hex without the <code>0x</code> prefix. Forge outputs bytecode with <code>0x</code>. <pre><code># Fix: strip the prefix\nforge inspect TestableVault bytecode | sed 's/^0x//'\n</code></pre></p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#problem-2-runtime-vs-creation-bytecode","title":"Problem 2: Runtime vs Creation Bytecode","text":"<p><code>forge inspect TestableVault deployedBytecode</code> gives runtime bytecode \u2014 but Echidna's <code>deployBytecodes</code> needs creation bytecode (the constructor-bearing version) so it can actually deploy. <pre><code># Correct: use creation bytecode\nforge inspect TestableVault bytecode | sed 's/^0x//'\n# Wrong: runtime bytecode\nforge inspect TestableVault deployedBytecode | sed 's/^0x//'\n</code></pre></p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#problem-3-balanceaddrbalancecontract-crash","title":"Problem 3: balanceAddr/balanceContract Crash","text":"<p><code>balanceAddr: 0xFFFFFFFFFFFFFFFF</code> and <code>balanceContract: 0xFFFFFFFFFFFFFFFF</code> in <code>echidna.yaml</code> caused silent deployment failure \u2014 the contract at the target address just didn't exist. Found via incremental config testing (adding one YAML option at a time). Solution: remove both options entirely.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#problem-4-ownership-chain","title":"Problem 4: Ownership Chain","text":"<p><code>EchidnaSetupLight</code> (the helper contract deployed via <code>deployContracts</code>) needs to: 1. Deploy <code>VariableVaultFee</code> with <code>address(this)</code> as owner (so it can call <code>registerAsset()</code>) 2. Call <code>registerAsset()</code> to configure the fee contract 3. Transfer ownership to the actual deployer (<code>0x10000</code>) 4. Same pattern for <code>MockERC20</code> \u2014 deploy, then <code>transferOwnership(owner)</code></p> <pre><code>// In EchidnaSetup constructor:\nasset = new MockERC20(\"USDC\", \"USDC\", 6);\nasset.transferOwnership(owner);\nfeeContract = new VariableVaultFee(address(this)); // self-owned temporarily\nfeeContract.registerAsset(...);\nfeeContract.transferOwnership(owner); // hand off\n</code></pre>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#problem-5-testlimit-0-unlimited","title":"Problem 5: testLimit: 0 \u2260 Unlimited","text":"<p>Setting <code>testLimit: 0</code> means zero tests, not infinite. Echidna workers immediately stop with <code>fuzzing: 1304/0</code>. Use a large number like <code>testLimit: 999999999</code> or a reasonable <code>500000</code>.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#problem-6-deployer-must-be-a-sender","title":"Problem 6: Deployer Must Be a Sender","text":"<p>If the deployer address (<code>0x10000</code>) isn't in the <code>sender</code> list, Echidna can't call setup functions. Always include the deployer in senders.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#final-working-config","title":"Final Working Config","text":"<pre><code>testMode: \"property\"\ntestLimit: 500000\nshrinkLimit: 5000\nseqLen: 100\ndeployer: \"0x10000\"\nsender:\n  - \"0x10000\"\n  - \"0x20000\"\n  - \"0x30000\"\ncorpusDir: \"echidna-corpus\"\ncodeSize: 0xFFFFFF\nworkers: 4\nformat: text\ndeployContracts:\n  - [\"0x00000000000000000000000000000000000AAAAA\", \"EchidnaSetupLight\"]\ndeployBytecodes:\n  - [\"0x00000000000000000000000000000000000BBBBB\", \"&lt;creation bytecode&gt;\"]\n</code></pre>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#fuzzing-round-1-strict-properties-6-properties-false-positives","title":"Fuzzing Round 1: Strict Properties (6 Properties, False Positives)","text":"<p>First attempt used 6 strict invariants. 5 of 6 failed \u2014 all false positives caused by <code>adminMint()</code> creating unbacked shares:</p> <ol> <li><code>echidna_share_price_gte_one</code> \u2014 FAILED: adminMint inflates supply without backing</li> <li><code>echidna_no_free_shares</code> \u2014 FAILED: adminMint gives shares for free by design</li> <li><code>echidna_conversion_rounding_favors_vault</code> \u2014 PASSED</li> <li><code>echidna_solvency</code> \u2014 FAILED: adminMint makes vault insolvent for existing holders</li> <li><code>echidna_no_zero_share_deposit</code> \u2014 FAILED: after adminMint inflates supply, small deposits round to 0 shares</li> <li><code>echidna_total_supply_consistency</code> \u2014 FAILED: ghost tracking didn't account for admin operations</li> </ol> <p>Lesson: <code>adminMint</code>/<code>adminBurn</code> are intentional privileged operations that break naive accounting invariants. They should be excluded from fuzzer actions when testing economic properties.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#fuzzing-round-2-realistic-properties-13-properties-500k-calls","title":"Fuzzing Round 2: Realistic Properties (13 Properties, 500K Calls)","text":"<p>Rewrote the harness excluding <code>adminMint</code>/<code>adminBurn</code> from action functions. Added ghost variable tracking. Used minimum deposit of <code>1e6</code> (1 USDC) instead of <code>1</code> wei.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#action-functions","title":"Action Functions","text":"<ul> <li><code>action_deposit(uint256)</code> \u2014 deposit as msg.sender, clamp to [1e6, balance]</li> <li><code>action_deposit_for_user(uint256)</code> \u2014 deposit for address <code>0x20000</code></li> <li><code>action_mint_shares(uint256)</code> \u2014 mint shares as msg.sender</li> <li><code>action_request_redeem(uint256)</code> \u2014 request async redeem</li> <li><code>action_fulfill_redeem(uint256)</code> \u2014 admin fulfills pending redeem</li> <li><code>action_cancel_redeem(uint256)</code> \u2014 admin cancels pending redeem</li> <li><code>action_update_balance(uint256)</code> \u2014 oracle updates underlying balance (\u00b120%)</li> <li><code>action_donate_to_vault(uint256)</code> \u2014 direct ERC20 transfer to vault (inflation attack vector)</li> </ul>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#ghost-variables","title":"Ghost Variables","text":"<pre><code>uint256 public totalDeposited;       // cumulative assets deposited\nuint256 public totalSharesMinted;     // cumulative shares received from deposits\nuint256 public totalAssetsWithdrawn;  // cumulative assets from fulfilled redeems\nuint256 public totalSharesBurned;     // cumulative shares burned in redeems\nuint256 public totalSharesCancelled;  // cumulative shares returned from cancelled redeems\nuint256 public ghostPendingShares;    // running total of pending redeem shares\n</code></pre>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#results-9-passing-4-failed","title":"Results: 9 Passing, 4 Failed","text":"<p>PASSING (9): 1. <code>echidna_roundtrip_rounding</code> \u2014 convertToAssets(convertToShares(x)) &lt;= x \u2713 2. <code>echidna_deposit_preview_ge_convert</code> \u2014 previewDeposit(x) &gt;= convertToShares(x) \u2713 3. <code>echidna_vault_holds_pending</code> \u2014 vault's own share balance &gt;= ghostPendingShares \u2713 4. <code>echidna_supply_conservation</code> \u2014 totalSupply changes only through deposit/mint/redeem/cancel \u2713 5. <code>echidna_pending_consistency</code> \u2014 totalPendingAssets only increases on requestRedeem, decreases on fulfill/cancel \u2713 6. <code>echidna_no_deposit_redeem_profit</code> \u2014 no instant deposit\u2192requestRedeem profit \u2713 7. <code>echidna_total_assets_ge_balance</code> \u2014 totalAssets() &gt;= asset.balanceOf(vault) \u2713 (with oracle) 8. <code>echidna_pending_assets_nonneg</code> \u2014 totalPendingAssets &gt;= 0 (can't underflow) \u2713 9. <code>echidna_pause_consistency</code> \u2014 paused state correctly blocks operations \u2713</p> <p>FAILED (4) \u2014 All Explained:</p> <ol> <li> <p><code>echidna_share_accounting</code> \u2014 Harness bug, not protocol bug. Ghost variable <code>totalSharesMinted</code> was incremented for <code>address(this)</code> but deposits went to <code>0x20000</code>. Fix: track per-recipient.</p> </li> <li> <p><code>echidna_no_zero_share_for_real_deposit</code> \u2014 Donation inflation attack. Attacker donates tokens directly to vault, inflating share price so victim's deposit rounds to 0 shares. Requires ~10 billion USDC to steal meaningful amounts from a 1 USDC deposit. Not practical.</p> </li> <li> <p><code>echidna_exchange_rate_ge_one</code> \u2014 Off-by-1 after oracle update. See rounding analysis below.</p> </li> <li> <p><code>echidna_mint_deposit_inverse</code> \u2014 previewMint/previewDeposit asymmetry. See rounding analysis below.</p> </li> </ol>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#rounding-analysis-deep-dive","title":"Rounding Analysis (Deep Dive)","text":"<p>All three rounding findings are expected ERC4626 behavior, not bugs.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#finding-1-exchange-rate-off-by-1-after-oracle-update","title":"Finding 1: Exchange Rate Off-by-1 After Oracle Update","text":"<p>What happens: After <code>onUnderlyingBalanceUpdate()</code> changes <code>totalAssets</code>, the invariant <code>convertToAssets(totalSupply) &gt;= totalSupply</code> can fail by exactly 1 wei.</p> <p>Math trace: <pre><code>State: totalAssets = 1000007, totalSupply = 1000000\n\nconvertToAssets(totalSupply) = totalSupply * totalAssets / totalSupply\n                              = 1000000 * 1000007 / 1000000\n</code></pre></p> <p>In Solidity integer division: <pre><code>1000000 * 1000007 = 1000007000000\n1000007000000 / 1000000 = 1000007  (exact, no remainder)\n</code></pre></p> <p>But with non-round numbers like <code>totalAssets = 1000003, totalSupply = 999997</code>: <pre><code>convertToAssets(999997) = 999997 * 1000003 / 999997 = 1000003 (exact here)\n</code></pre></p> <p>The failure happens when oracle updates create a <code>totalAssets</code> that doesn't divide evenly: <pre><code>totalAssets = 2000001, totalSupply = 2000000\nconvertToAssets(2000000) = 2000000 * 2000001 / 2000000 = 2000001 \u2713\n\nBut after fee + rounding chain:\nactualAssets stored = 1999999 (due to fee rounding)\ntotalSupply still = 2000000\nconvertToAssets(2000000) = 2000000 * 1999999 / 2000000 = 1999999 &lt; 2000000 \u2717\n</code></pre></p> <p>Verdict: This is inherent to integer math. The vault's <code>totalAssets</code> after oracle updates doesn't guarantee exact divisibility with <code>totalSupply</code>. The 1 wei discrepancy is economically meaningless and is standard ERC4626 behavior.</p> <p>Severity: Informational / Not a bug.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#finding-2-previewmint-vs-previewdeposit-asymmetry","title":"Finding 2: previewMint vs previewDeposit Asymmetry","text":"<p>What happens: <code>previewMint(convertToShares(assets))</code> can return <code>assets + 1</code>, violating a naive \"roundtrip\" expectation.</p> <p>Math trace: <pre><code>State: totalAssets = 1000003, totalSupply = 1000000\n\nStep 1: convertToShares(1000) = 1000 * 1000000 / 1000003\n        = 1000000000 / 1000003\n        = 999 (truncated \u2014 rounds DOWN, fewer shares, favors vault)\n\nStep 2: previewMint(999) = 999 * 1000003 / 1000000 (rounds UP via mulDivUp)\n        = 999002997 / 1000000\n        = 999.002997 \u2192 rounds UP to 1000\n</code></pre></p> <p>So depositing 1000 assets gets you 999 shares, but minting 999 shares costs 1000 assets.</p> <p>Why this is correct: - <code>previewDeposit</code> / <code>convertToShares</code>: rounds DOWN (you get fewer shares \u2192 vault keeps dust) - <code>previewMint</code> / <code>convertToAssets</code>: rounds UP (you pay more assets \u2192 vault keeps dust) - Both directions favor the vault, preventing share dilution attacks - This is the OpenZeppelin ERC4626 standard implementation</p> <p>Severity: Informational / By design.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#finding-3-donation-inflation-zero-share-deposit","title":"Finding 3: Donation Inflation (Zero-Share Deposit)","text":"<p>What happens: An attacker can: 1. Deposit 1 wei to get 1 share 2. Donate a huge amount of tokens directly to the vault via <code>ERC20.transfer()</code> 3. This inflates <code>totalAssets</code> without increasing <code>totalSupply</code> 4. A victim's subsequent deposit gets <code>amount * 1 / (1 + huge_donation)</code> \u2192 rounds to 0 shares</p> <p>Math trace: <pre><code>Attacker deposits 1 wei \u2192 gets 1 share\nAttacker donates 10,000,000,000e6 USDC to vault\n\nState: totalAssets = 10_000_000_001e6, totalSupply = 1\n\nVictim deposits 1e6 (1 USDC):\nshares = 1e6 * 1 / 10_000_000_001e6 = 0 (truncated)\n</code></pre></p> <p>Why it's not practical: - Requires mass of ~$10 billion USDC to attack a $1 deposit - The attacker loses the donated funds (they're trapped in the vault backing the 1 share) - MultipliVault also has admin controls and minimum deposit thresholds that mitigate this - OpenZeppelin recommends \"virtual shares\" (ERC4626 with offset) for vaults that need protection, but the economic cost here makes it irrelevant</p> <p>Severity: Informational / Not practical. Standard known ERC4626 consideration.</p>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#key-files","title":"Key Files","text":"<ul> <li><code>test/echidna/EchidnaMultipliVault.sol</code> \u2014 Main fuzzing harness with 13 realistic properties and ghost variable tracking</li> <li><code>test/echidna/EchidnaSetup.sol</code> \u2014 Setup helper deploying MockERC20, VariableVaultFee, MockAuthority with correct ownership chain</li> <li><code>test/echidna/TestableVault.sol</code> \u2014 MultipliVault subclass without <code>_disableInitializers()</code> (needed for Echidna deployment)</li> <li><code>echidna.yaml</code> \u2014 Working Echidna configuration</li> <li><code>src/vault/MultipliVault.sol</code> \u2014 Main target contract (UUPS upgradeable ERC4626 with async redeem)</li> <li><code>src/fees/VariableVaultFee.sol</code> \u2014 Fee calculation logic</li> </ul>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]},{"location":"2026/2026-02/2026-02-27-multiplivault-echidna-fuzzing-rounding-a-c86fa4ad/#run-command","title":"Run Command","text":"<pre><code>echidna test/echidna/EchidnaMultipliVault.sol --contract EchidnaMultipliVault --config echidna.yaml\n</code></pre>","tags":["echidna","fuzzing","solidity","erc4626","rounding","bug-bounty","hackenproof","avalanche","integer-math","inflation-attack"]}]}